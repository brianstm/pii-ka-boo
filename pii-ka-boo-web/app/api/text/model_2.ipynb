{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7c6bf37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.55.4\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ed10cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Winston\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:61: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: ./pii_dataset.csv\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './pii_dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m csv_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./pii_dataset.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing:\u001b[39m\u001b[38;5;124m\"\u001b[39m, csv_path)\n\u001b[1;32m----> 8\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(csv_path)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, df\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumns:\u001b[39m\u001b[38;5;124m\"\u001b[39m, df\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist())\n",
      "File \u001b[1;32mc:\\Users\\Winston\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\Winston\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\Winston\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32mc:\\Users\\Winston\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\Winston\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './pii_dataset.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Adjust this if your filename differs\n",
    "csv_path = \"./pii_dataset.csv\"\n",
    "print(\"Using:\", csv_path)\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "\n",
    "# Peek at a few rows\n",
    "print(df.head(2).to_dict(orient=\"records\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a928b14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token column candidates: ['tokens']\n",
      "Label column candidates: ['labels']\n",
      "Row 0 token count vs label count: 363 vs 363\n",
      "First 10 tokens: ['My', 'name', 'is', 'Aaliyah', 'Popova,', 'and', 'I', 'am', 'a', 'jeweler']\n",
      "First 10 BIO tags: ['O', 'O', 'O', 'B-NAME_STUDENT', 'I-NAME_STUDENT', 'O', 'O', 'O', 'O', 'O']\n",
      "Rows with length mismatch: 0 out of 4434\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# If you haven't already loaded df:\n",
    "# df = pd.read_csv(\"<your path>.csv\")\n",
    "\n",
    "# 1) Find likely columns for tokens and labels\n",
    "tok_col_candidates = [c for c in df.columns if c.lower() in [\"tokens\",\"words\",\"tokens_list\",\"tokens_str\"]]\n",
    "lab_col_candidates = [c for c in df.columns if c.lower() in [\"labels\",\"ner_tags\",\"tags\"]]\n",
    "\n",
    "print(\"Token column candidates:\", tok_col_candidates)\n",
    "print(\"Label column candidates:\", lab_col_candidates)\n",
    "\n",
    "TOK_COL = tok_col_candidates[0]   # adjust if needed\n",
    "LAB_COL = lab_col_candidates[0]\n",
    "\n",
    "# 2) Ensure labels are a Python list (some CSVs save them as strings)\n",
    "def to_list(x):\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            return ast.literal_eval(x)\n",
    "        except Exception:\n",
    "            return x.split()  # last resort: space-separated\n",
    "    return list(x)\n",
    "\n",
    "df[\"TOKENS\"] = df[TOK_COL].apply(to_list)\n",
    "df[\"BIO\"]    = df[LAB_COL].apply(to_list)\n",
    "\n",
    "# 3) Check alignment and peek\n",
    "i = 0  # try a few different rows if needed\n",
    "print(\"Row\", i, \"token count vs label count:\",\n",
    "      len(df.loc[i, \"TOKENS\"]), \"vs\", len(df.loc[i, \"BIO\"]))\n",
    "\n",
    "print(\"First 10 tokens:\", df.loc[i, \"TOKENS\"][:10])\n",
    "print(\"First 10 BIO tags:\", df.loc[i, \"BIO\"][:10])\n",
    "\n",
    "# 4) How many rows mismatch?\n",
    "mismatch = (df[\"TOKENS\"].str.len() != df[\"BIO\"].str.len()).sum()\n",
    "print(\"Rows with length mismatch:\", mismatch, \"out of\", len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bc6386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatches after mapping: 0\n",
      "BIO6 label distribution (top 15): [('O', 1333514), ('B-NAME', 11104), ('I-ADDRESS', 8577), ('I-NAME', 5667), ('B-EMAIL', 3794), ('B-ADDRESS', 3543), ('I-PHONE', 3389), ('B-PHONE', 2419), ('B-USERNAME', 718), ('B-URL_PERSONAL', 620)]\n",
      "[('My', 'O', 'O'), ('name', 'O', 'O'), ('is', 'O', 'O'), ('Aaliyah', 'B-NAME_STUDENT', 'B-NAME'), ('Popova,', 'I-NAME_STUDENT', 'I-NAME'), ('and', 'O', 'O'), ('I', 'O', 'O'), ('am', 'O', 'O'), ('a', 'O', 'O'), ('jeweler', 'O', 'O'), ('with', 'O', 'O'), ('13', 'O', 'O'), ('years', 'O', 'O'), ('of', 'O', 'O'), ('experience.', 'O', 'O'), ('I', 'O', 'O'), ('remember', 'O', 'O'), ('a', 'O', 'O'), ('very', 'O', 'O'), ('unique', 'O', 'O')]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# 1) Raw -> canonical mapping (now includes USERNAME, URL_PERSONAL)\n",
    "RAW2CANON = {\n",
    "    # Names\n",
    "    \"NAME_STUDENT\": \"NAME\", \"NAME\": \"NAME\", \"PERSON\": \"NAME\",\n",
    "    # Emails\n",
    "    \"EMAIL\": \"EMAIL\", \"EMAIL_ADDRESS\": \"EMAIL\",\n",
    "    # Phones\n",
    "    \"PHONE_NUM\": \"PHONE\", \"PHONE\": \"PHONE\", \"PHONE_NUMBER\": \"PHONE\",\n",
    "    # Addresses\n",
    "    \"STREET_ADDRESS\": \"ADDRESS\", \"ADDRESS\": \"ADDRESS\",\n",
    "    # Extras you asked to keep\n",
    "    \"USERNAME\": \"USERNAME\",\n",
    "    \"URL_PERSONAL\": \"URL_PERSONAL\",\n",
    "}\n",
    "\n",
    "TARGET_ENTS = {\"NAME\",\"EMAIL\",\"PHONE\",\"ADDRESS\",\"USERNAME\",\"URL_PERSONAL\"}\n",
    "\n",
    "def map_bio_tag(tag: str) -> str:\n",
    "    if tag == \"O\":\n",
    "        return \"O\"\n",
    "    if \"-\" not in tag:  # unexpected form\n",
    "        return \"O\"\n",
    "    prefix, raw = tag.split(\"-\", 1)  # e.g., 'B', 'NAME_STUDENT'\n",
    "    canon = RAW2CANON.get(raw)\n",
    "    if canon in TARGET_ENTS:\n",
    "        return f\"{prefix}-{canon}\"\n",
    "    return \"O\"\n",
    "\n",
    "# 2) Apply mapping\n",
    "df[\"BIO6\"] = df[\"BIO\"].apply(lambda seq: [map_bio_tag(t) for t in seq])\n",
    "\n",
    "# 3) Sanity checks\n",
    "mismatch_after = (df[\"TOKENS\"].str.len() != df[\"BIO6\"].str.len()).sum()\n",
    "print(\"Mismatches after mapping:\", mismatch_after)\n",
    "\n",
    "bio6_types = Counter()\n",
    "for tags in df[\"BIO6\"]:\n",
    "    bio6_types.update(tags)\n",
    "print(\"BIO6 label distribution (top 15):\", bio6_types.most_common(15))\n",
    "\n",
    "# 4) Quick peek\n",
    "i = 0\n",
    "print(list(zip(df.loc[i,\"TOKENS\"][:20], df.loc[i,\"BIO\"][:20], df.loc[i,\"BIO6\"][:20])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40780644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label vocab size: 13\n",
      "Unknown tags (should be empty): []\n",
      "Sample mapping: {'O': 0, 'B-NAME': 1, 'I-NAME': 2, 'B-EMAIL': 3, 'I-EMAIL': 4, 'B-PHONE': 5, 'I-PHONE': 6, 'B-ADDRESS': 7}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "ENTITIES = [\"NAME\",\"EMAIL\",\"PHONE\",\"ADDRESS\",\"USERNAME\",\"URL_PERSONAL\"]\n",
    "BIO_LABELS = [\"O\"] + [f\"{p}-{e}\" for e in ENTITIES for p in [\"B\",\"I\"]]\n",
    "label2id = {l:i for i,l in enumerate(BIO_LABELS)}\n",
    "id2label = {i:l for l,i in label2id.items()}\n",
    "\n",
    "# Verify: any tags outside this set?\n",
    "seen = Counter(t for row in df[\"BIO6\"] for t in row)\n",
    "unknown = [t for t in seen if t not in BIO_LABELS]\n",
    "print(\"Label vocab size:\", len(BIO_LABELS))\n",
    "print(\"Unknown tags (should be empty):\", unknown[:10])\n",
    "print(\"Sample mapping:\", {k:label2id[k] for k in BIO_LABELS[:8]})\n",
    "#“Sample mapping” is just a peek at the dictionary that converts your human-readable BIO tags into the integer IDs the model actually trains on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a98a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'tags'],\n",
      "        num_rows: 3990\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'tags'],\n",
      "        num_rows: 444\n",
      "    })\n",
      "})\n",
      "Train rows: 3990 | Val rows: 444\n",
      "One row example: dict_keys(['tokens', 'tags'])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "ds_all = Dataset.from_pandas(\n",
    "    df[[\"TOKENS\",\"BIO6\"]].rename(columns={\"TOKENS\":\"tokens\",\"BIO6\":\"tags\"}),\n",
    "    preserve_index=False\n",
    ")\n",
    "\n",
    "# simple random split (90/10)\n",
    "N = len(ds_all)\n",
    "idx = np.arange(N)\n",
    "rng = np.random.default_rng(42)\n",
    "rng.shuffle(idx)\n",
    "cut = int(0.9*N)\n",
    "train_idx, val_idx = idx[:cut], idx[cut:]\n",
    "\n",
    "ds = DatasetDict({\n",
    "    \"train\": ds_all.select(train_idx.tolist()),\n",
    "    \"validation\": ds_all.select(val_idx.tolist()),\n",
    "})\n",
    "\n",
    "print(ds)\n",
    "print(\"Train rows:\", ds[\"train\"].num_rows, \"| Val rows:\", ds[\"validation\"].num_rows)\n",
    "print(\"One row example:\", ds[\"train\"][0].keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6335b49d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1cbe8d1718f4ca68232d792e225a602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing and aligning labels:   0%|          | 0/3990 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d37aa67d2164b3bb601c85d72e86d2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing and aligning labels:   0%|          | 0/444 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 3990\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 444\n",
      "    })\n",
      "})\n",
      "Keys in encoded batch: dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "len(input_ids) vs len(labels): 475 475\n",
      "Active labels in example row: 347\n",
      "First 20 labels (ids): [-100, 0, 0, 0, 0, 0, 0, 0, 0, -100, 1, 2, -100, 0, 0, 0, 0, 0, -100, 0]\n",
      "First 20 label names: ['PAD', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'PAD', 'B-NAME', 'I-NAME', 'PAD', 'O', 'O', 'O', 'O', 'O', 'PAD', 'O']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
    "\n",
    "def encode_batch(batch):\n",
    "    # Tokenize list-of-words with alignment info\n",
    "    enc = tokenizer(\n",
    "        batch[\"tokens\"],\n",
    "        is_split_into_words=True,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        max_length=512,\n",
    "    )\n",
    "    all_labels = []\n",
    "    for i, tags in enumerate(batch[\"tags\"]):\n",
    "        word_ids = enc.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        prev_wid = None\n",
    "        for wid in word_ids:\n",
    "            if wid is None:\n",
    "                label_ids.append(-100)            # special tokens\n",
    "            elif wid != prev_wid:\n",
    "                label_ids.append(label2id[tags[wid]])  # first subword of this word\n",
    "            else:\n",
    "                label_ids.append(-100)            # subsequent subwords\n",
    "            prev_wid = wid\n",
    "        all_labels.append(label_ids)\n",
    "    enc[\"labels\"] = all_labels\n",
    "    return enc\n",
    "\n",
    "encoded = ds.map(\n",
    "    encode_batch,\n",
    "    batched=True,\n",
    "    remove_columns=[\"tokens\",\"tags\"],\n",
    "    desc=\"Tokenizing and aligning labels\",\n",
    ")\n",
    "\n",
    "print(encoded)\n",
    "row = encoded[\"train\"][0]\n",
    "print(\"Keys in encoded batch:\", row.keys())\n",
    "print(\"len(input_ids) vs len(labels):\", len(row[\"input_ids\"]), len(row[\"labels\"]))\n",
    "# How many labels are active (i.e., not -100) in this example?\n",
    "active = sum(1 for x in row[\"labels\"] if x != -100)\n",
    "print(\"Active labels in example row:\", active)\n",
    "print(\"First 20 labels (ids):\", row[\"labels\"][:20])\n",
    "print(\"First 20 label names:\", [id2label[i] if i!=-100 else \"PAD\" for i in row[\"labels\"][:20]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506e6499",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96aecfbed36a48ada3f354886ef922d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenize (speed mode):   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5261098f92524a3184750ea94f4cc0ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenize (speed mode):   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Winston\\AppData\\Local\\Temp\\ipykernel_42720\\1479944800.py:100: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer ready (speed mode).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='47' max='47' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [47/47 05:42, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training summary (speed mode) ===\n",
      "TrainOutput(global_step=47, training_loss=0.23462857591344954, metrics={'train_runtime': 380.3809, 'train_samples_per_second': 3.943, 'train_steps_per_second': 0.124, 'total_flos': 49004657280000.0, 'train_loss': 0.23462857591344954, 'epoch': 1.0})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Winston\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Eval metrics (validation, speed mode) ===\n",
      "eval_f1_ADDRESS: 0.8056\n",
      "eval_f1_EMAIL: 0.0000\n",
      "eval_f1_NAME: 0.6796\n",
      "eval_f1_PHONE: 0.0000\n",
      "eval_f1_URL_PERSONAL: 0.0000\n",
      "eval_f1_USERNAME: 0.0000\n",
      "eval_loss: 0.0326\n",
      "eval_overall_f1: 0.6621\n",
      "eval_overall_precision: 0.6175\n",
      "eval_overall_recall: 0.7137\n",
      "eval_runtime: 50.4945\n",
      "eval_samples_per_second: 5.9410\n",
      "eval_steps_per_second: 0.1980\n",
      "\n",
      "Model saved to: pii-ner-fast\n"
     ]
    }
   ],
   "source": [
    "# --- SPEED MODE SWITCHES ---\n",
    "MAX_LEN = 128            # shorter sequences = much faster\n",
    "TRAIN_ROWS = 1500        # small train subset\n",
    "VAL_ROWS = 300           # small val subset\n",
    "EPOCHS = 1               # quick pass\n",
    "BATCH = 32               # try 24/32; lower if you hit RAM issues\n",
    "\n",
    "# 1) Small, fast backbone\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForTokenClassification\n",
    "MODEL_NAME = \"distilbert-base-cased\"  # tiny & quick\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "config = AutoConfig.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(BIO_LABELS),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "model = AutoModelForTokenClassification.from_pretrained(MODEL_NAME, config=config)\n",
    "\n",
    "# 2) Re-tokenize with shorter max_length and first-subword labeling\n",
    "def encode_batch(batch):\n",
    "    enc = tokenizer(\n",
    "        batch[\"tokens\"],\n",
    "        is_split_into_words=True,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        max_length=MAX_LEN,\n",
    "    )\n",
    "    all_labels = []\n",
    "    for i, tags in enumerate(batch[\"tags\"]):\n",
    "        word_ids = enc.word_ids(batch_index=i)\n",
    "        label_ids, prev_wid = [], None\n",
    "        for wid in word_ids:\n",
    "            if wid is None:\n",
    "                label_ids.append(-100)\n",
    "            elif wid != prev_wid:\n",
    "                label_ids.append(label2id[tags[wid]])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            prev_wid = wid\n",
    "        all_labels.append(label_ids)\n",
    "    enc[\"labels\"] = all_labels\n",
    "    return enc\n",
    "\n",
    "from datasets import DatasetDict\n",
    "encoded_fast = DatasetDict({\n",
    "    \"train\": ds[\"train\"].select(range(min(TRAIN_ROWS, ds[\"train\"].num_rows))),\n",
    "    \"validation\": ds[\"validation\"].select(range(min(VAL_ROWS, ds[\"validation\"].num_rows))),\n",
    "}).map(encode_batch, batched=True, remove_columns=[\"tokens\",\"tags\"], desc=\"Tokenize (speed mode)\")\n",
    "\n",
    "# 3) Compat-safe Trainer args (only pass what your version supports)\n",
    "from transformers import DataCollatorForTokenClassification, TrainingArguments, Trainer\n",
    "import inspect, numpy as np, evaluate\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=2)\n",
    "    true_preds, true_labels = [], []\n",
    "    for pred, lab in zip(preds, p.label_ids):\n",
    "        keep = lab != -100\n",
    "        true_preds.append([id2label[int(i)] for i in pred[keep]])\n",
    "        true_labels.append([id2label[int(i)] for i in lab[keep]])\n",
    "    res = seqeval.compute(predictions=true_preds, references=true_labels, zero_division=0)\n",
    "    out = {\n",
    "        \"overall_precision\": res.get(\"overall_precision\", 0.0),\n",
    "        \"overall_recall\": res.get(\"overall_recall\", 0.0),\n",
    "        \"overall_f1\": res.get(\"overall_f1\", 0.0),\n",
    "    }\n",
    "    for ent, st in res.items():\n",
    "        if isinstance(st, dict) and \"f1\" in st:\n",
    "            out[f\"f1_{ent}\"] = st[\"f1\"]\n",
    "    return out\n",
    "\n",
    "sig = inspect.signature(TrainingArguments.__init__)\n",
    "params = set(sig.parameters.keys())\n",
    "\n",
    "kwargs = dict(\n",
    "    output_dir=\"pii-ner-fast\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=BATCH,\n",
    "    per_device_eval_batch_size=BATCH,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=0.0,      # less overhead\n",
    "    logging_steps=100,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# add optional args only if your installed version supports them\n",
    "if \"dataloader_num_workers\" in params:\n",
    "    kwargs[\"dataloader_num_workers\"] = 2\n",
    "if \"report_to\" in params:\n",
    "    kwargs[\"report_to\"] = \"none\"\n",
    "# we skip eval/save strategies during training; we’ll eval once after\n",
    "\n",
    "args = TrainingArguments(**kwargs)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=encoded_fast[\"train\"],\n",
    "    eval_dataset=encoded_fast[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Trainer ready (speed mode).\")\n",
    "\n",
    "# 4) Train quick, then evaluate once\n",
    "train_out = trainer.train()\n",
    "print(\"\\n=== Training summary (speed mode) ===\")\n",
    "print(train_out)\n",
    "\n",
    "eval_out = trainer.evaluate()\n",
    "print(\"\\n=== Eval metrics (validation, speed mode) ===\")\n",
    "for k, v in sorted(eval_out.items()):\n",
    "    if k.startswith(\"eval_\"):\n",
    "        print(f\"{k}: {v:.4f}\" if isinstance(v, (int,float)) else f\"{k}: {v}\")\n",
    "\n",
    "# Save a checkpoint you can test with Step 8\n",
    "trainer.save_model(\"pii-ner-fast\")\n",
    "tokenizer.save_pretrained(\"pii-ner-fast\")\n",
    "print(\"\\nModel saved to: pii-ner-fast\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee54320a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Winston\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:61: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tags: ['O', 'O', 'O', 'B-NAME', 'B-NAME', 'B-NAME', 'B-NAME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "offs: [(0, 2), (3, 7), (8, 10), (11, 18), (19, 26), (27, 28), (28, 31), (31, 34), (34, 36), (36, 37), (38, 40), (41, 48), (49, 51), (52, 57), (57, 58), (58, 59), (59, 60), (60, 67), (67, 68), (68, 71), (71, 72), (72, 76), (76, 79), (79, 80), (81, 84), (85, 88), (89, 94), (95, 97), (98, 100)]\n",
      "\n",
      "IN : My name is Winston Leonard Prayonggo. My website is https://example.com/winston. You can reach me at\n",
      "OUT: My name is [NAME]. My website is [URL_PERSONAL] You can reach me at\n",
      "SPN: [{'start': 11, 'end': 36, 'label': 'NAME'}, {'start': 52, 'end': 80, 'label': 'URL_PERSONAL'}]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import sys\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "\n",
    "MODEL_DIR = \"./app/api/text/pii-ner-fast\"\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "mdl = AutoModelForTokenClassification.from_pretrained(MODEL_DIR)\n",
    "mdl.eval()\n",
    "\n",
    "def extend_spans_to_word_end(spans, text, labels={\"NAME\", \"ADDRESS\"}):\n",
    "    \"\"\"Extend spans to cover complete words.\"\"\"\n",
    "    out = []\n",
    "    for s in spans:\n",
    "        if s[\"label\"] in labels:\n",
    "            end = s[\"end\"]\n",
    "            L = len(text)\n",
    "            while end < L and re.match(r\"[A-Za-z0-9'’-]\", text[end]):\n",
    "                end += 1\n",
    "            s = {\"start\": s[\"start\"], \"end\": end, \"label\": s[\"label\"]}\n",
    "        out.append(s)\n",
    "    return out\n",
    "\n",
    "def coalesce_same_label_spans(spans, text, max_gap_chars=2):\n",
    "    \"\"\"Merge adjacent spans with the same label.\"\"\"\n",
    "    if not spans:\n",
    "        return spans\n",
    "        \n",
    "    spans = sorted(spans, key=lambda s: (s[\"start\"], s[\"end\"]))\n",
    "    merged = [spans[0]]\n",
    "\n",
    "    for s in spans[1:]:\n",
    "        prev = merged[-1]\n",
    "        if s[\"label\"] == prev[\"label\"]:\n",
    "            gap = text[prev[\"end\"]:s[\"start\"]]\n",
    "            if re.fullmatch(rf\"\\s*[-/#A-Za-z0-9]{{0,{max_gap_chars}}}\\s*\", gap):\n",
    "                prev[\"end\"] = s[\"end\"]\n",
    "                continue\n",
    "        merged.append(s)\n",
    "    return merged\n",
    "\n",
    "def predict_tags(text):\n",
    "    \"\"\"Predict named entities using the transformer model.\"\"\"\n",
    "    enc = tok(text, return_offsets_mapping=True, return_tensors=\"pt\", \n",
    "              truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        out = mdl(input_ids=enc[\"input_ids\"], \n",
    "                 attention_mask=enc[\"attention_mask\"])\n",
    "    pred_ids = out.logits.argmax(-1)[0].tolist()\n",
    "    tags = [mdl.config.id2label[int(i)] for i in pred_ids]\n",
    "\n",
    "    offsets = enc[\"offset_mapping\"][0].tolist()\n",
    "    clean_tags, clean_offs = [], []\n",
    "    for (s, e), tag in zip(offsets, tags):\n",
    "        if s == 0 and e == 0:\n",
    "            continue\n",
    "        clean_tags.append(tag)\n",
    "        clean_offs.append((s, e))\n",
    "    return clean_tags, clean_offs\n",
    "\n",
    "def bio_to_char_spans(offsets, tags):\n",
    "    \"\"\"Convert BIO tags to character spans.\"\"\"\n",
    "    spans, cur = [], None\n",
    "    for (s, e), tag in zip(offsets, tags):\n",
    "        if tag.startswith(\"B-\"):\n",
    "            if cur:\n",
    "                spans.append(cur)\n",
    "            cur = {\"start\": s, \"end\": e, \"label\": tag.split(\"-\", 1)[1]}\n",
    "        elif tag.startswith(\"I-\"):\n",
    "            ent = tag.split(\"-\", 1)[1]\n",
    "            if cur and cur[\"label\"] == ent and s <= cur[\"end\"] + 1:\n",
    "                cur[\"end\"] = e\n",
    "            else:\n",
    "                cur = {\"start\": s, \"end\": e, \"label\": ent}\n",
    "        else:\n",
    "            if cur:\n",
    "                spans.append(cur)\n",
    "                cur = None\n",
    "    if cur:\n",
    "        spans.append(cur)\n",
    "    return spans\n",
    "\n",
    "# Regex patterns for structured PII\n",
    "PII_PATTERNS = {\n",
    "    \"EMAIL\": re.compile(r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b\"),\n",
    "    \"PHONE\": re.compile(r\"\\b(?:\\+?\\d{1,3}[-.\\s]?)?(?:\\(?\\d{1,4}\\)?[-.\\s]?)?\\d{3,4}[-.\\s]?\\d{3,4}\\b\"),\n",
    "    \"URL_PERSONAL\": re.compile(r\"\\bhttps?://[^\\s]+\", re.I),\n",
    "    \"USERNAME\": re.compile(r\"@\\w{1,32}\")\n",
    "}\n",
    "\n",
    "def regex_spans(text, target_labels=None):\n",
    "    \"\"\"Extract PII spans using regex patterns, optionally filtered by target labels.\"\"\"\n",
    "    out = []\n",
    "    for label, pattern in PII_PATTERNS.items():\n",
    "        if target_labels and label not in target_labels:\n",
    "            continue\n",
    "        for m in pattern.finditer(text):\n",
    "            out.append({\"start\": m.start(), \"end\": m.end(), \"label\": label})\n",
    "    return out\n",
    "\n",
    "def ner_spans(text, target_labels=None):\n",
    "    \"\"\"Get named entity recognition spans, optionally filtered by target labels.\"\"\"\n",
    "    tags, offs = predict_tags(text)\n",
    "    spans = bio_to_char_spans(offs, tags)\n",
    "    \n",
    "    if target_labels:\n",
    "        spans = [span for span in spans if span[\"label\"] in target_labels]\n",
    "    \n",
    "    return spans\n",
    "\n",
    "def merge_spans(spans):\n",
    "    \"\"\"Merge overlapping spans.\"\"\"\n",
    "    spans = sorted(spans, key=lambda s: (s[\"start\"], -(s[\"end\"] - s[\"start\"])))\n",
    "    merged = []\n",
    "    for s in spans:\n",
    "        if not merged or s[\"start\"] >= merged[-1][\"end\"]:\n",
    "            merged.append(s)\n",
    "        else:\n",
    "            if (s[\"end\"] - s[\"start\"]) > (merged[-1][\"end\"] - merged[-1][\"start\"]):\n",
    "                merged[-1] = s\n",
    "    return merged\n",
    "\n",
    "def redact(text, target_labels=None):\n",
    "    ALL_LABELS = [\"NAME\", \"ADDRESS\", \"EMAIL\", \"PHONE\", \"URL_PERSONAL\", \"USERNAME\"]\n",
    "    if (target_labels != None):\n",
    "        target_labels = [item for item in ALL_LABELS if item not in target_labels]\n",
    "\n",
    "    \"\"\"\n",
    "    Redact PII entities with unique identifiers.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text to redact\n",
    "        target_labels: Set of labels to redact (e.g., {\"NAME\", \"PHONE\"}).\n",
    "                     If None, redacts all detected PII.\n",
    "    \"\"\"\n",
    "    # Get all available labels from both regex and NER\n",
    "    all_regex_labels = set(PII_PATTERNS.keys())\n",
    "    all_ner_labels = set(mdl.config.id2label.values())\n",
    "    all_ner_labels = {label.split(\"-\", 1)[1] for label in all_ner_labels if \"-\" in label}\n",
    "    \n",
    "    # If target_labels is specified, use only those labels\n",
    "    if target_labels:\n",
    "        target_labels = set(target_labels)\n",
    "        regex_matches = regex_spans(text, target_labels)\n",
    "        ner_matches = ner_spans(text, target_labels)\n",
    "    else:\n",
    "        # Redact all detected PII (original behavior)\n",
    "        regex_matches = regex_spans(text)\n",
    "        ner_matches = ner_spans(text)\n",
    "\n",
    "    keep = regex_matches[:]\n",
    "    covered = {(s[\"start\"], s[\"end\"]) for s in regex_matches}\n",
    "    \n",
    "    for s in ner_matches:\n",
    "        if (s[\"label\"] in {\"NAME\", \"ADDRESS\"} or \n",
    "            (s[\"start\"], s[\"end\"]) not in covered):\n",
    "            keep.append(s)\n",
    "\n",
    "    spans = merge_spans(keep)\n",
    "    spans = coalesce_same_label_spans(spans, text, max_gap_chars=2)\n",
    "    \n",
    "    # Only extend word endings for NAME and ADDRESS if they're in target labels\n",
    "    extend_labels = {\"NAME\", \"ADDRESS\"}\n",
    "    if target_labels:\n",
    "        extend_labels = extend_labels.intersection(target_labels)\n",
    "    spans = extend_spans_to_word_end(spans, text, labels=extend_labels)\n",
    "\n",
    "    # Count occurrences of each entity type\n",
    "    entity_counters = defaultdict(int)\n",
    "\n",
    "    for span in spans:\n",
    "        label = span[\"label\"]\n",
    "        entity_counters[label] += 1\n",
    "        span[\"entity_id\"] = f\"{label}_{entity_counters[label]}\"\n",
    "\n",
    "    # Build redacted text\n",
    "    out, last = [], 0\n",
    "    for s in sorted(spans, key=lambda x: x[\"start\"]):\n",
    "        out.append(text[last:s[\"start\"]])\n",
    "        out.append(f\"[{s['entity_id']}]\")\n",
    "        last = s[\"end\"]\n",
    "    out.append(text[last:])\n",
    "    \n",
    "    return \"\".join(out)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Read arguments from Node.js\n",
    "    if len(sys.argv) > 2:\n",
    "        # If specific labels are provided as second argument\n",
    "        text = sys.argv[1]\n",
    "        labels = json.loads(sys.argv[2])  # Expecting JSON array of labels\n",
    "        result = redact(text, target_labels=labels)\n",
    "    elif len(sys.argv) > 1:\n",
    "        # If only text is provided, redact all PII\n",
    "        text = sys.argv[1]\n",
    "        result = redact(text)\n",
    "    else:\n",
    "        result = \"No input provided\"\n",
    "    \n",
    "    print(json.dumps({\"response\": result}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
