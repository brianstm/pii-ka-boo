{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7c6bf37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.55.4\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ed10cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Winston\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:61: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: ./pii_dataset.csv\n",
      "Shape: (4434, 16)\n",
      "Columns: ['document', 'text', 'tokens', 'trailing_whitespace', 'labels', 'prompt', 'prompt_id', 'name', 'email', 'phone', 'job', 'address', 'username', 'url', 'hobby', 'len']\n",
      "[{'document': '1073d46f-2241-459b-ab01-851be8d26436', 'text': \"My name is Aaliyah Popova, and I am a jeweler with 13 years of experience. I remember a very unique and challenging project I had to work on last year. A customer approached me with a precious family heirloom - a diamond necklace that had been passed down through generations. Unfortunately, the necklace was in poor condition, with several loose diamonds and a broken clasp. The customer wanted me to restore it to its former glory, but it was clear that this would be no ordinary repair. Using my specialized tools and techniques, I began the delicate task of dismantling the necklace. Each diamond was carefully removed from its setting, and the damaged clasp was removed. Once the necklace was completely disassembled, I meticulously cleaned each diamond and inspected it for any damage. Fortunately, the diamonds were all in good condition, with no cracks or chips. The next step was to repair the broken clasp. I carefully soldered the broken pieces back together, ensuring that the clasp was sturdy and secure. Once the clasp was repaired, I began the process of reassembling the necklace. Each diamond was carefully placed back into its setting, and the necklace was polished until it sparkled like new. When I presented the restored necklace to the customer, they were overjoyed. They couldn't believe that I had been able to bring their family heirloom back to life. The necklace looked as beautiful as it had when it was first created, and the customer was thrilled to have it back in their possession. If you have a project that you would like to discuss, please feel free to contact me by phone at (95) 94215-7906 or by email at aaliyah.popova4783@aol.edu. I look forward to hearing from you! P.S.: When I'm not creating beautiful jewelry, I enjoy spending time podcasting. I love sharing my knowledge about jewelry and connecting with other people who are passionate about this art form. I also enjoy spending time with my family and exploring new places. If you would like to learn more about me, please feel free to visit my website at [website address] or visit me at my studio located at 97 Lincoln Street.\", 'tokens': '[\\'My\\', \\'name\\', \\'is\\', \\'Aaliyah\\', \\'Popova,\\', \\'and\\', \\'I\\', \\'am\\', \\'a\\', \\'jeweler\\', \\'with\\', \\'13\\', \\'years\\', \\'of\\', \\'experience.\\', \\'I\\', \\'remember\\', \\'a\\', \\'very\\', \\'unique\\', \\'and\\', \\'challenging\\', \\'project\\', \\'I\\', \\'had\\', \\'to\\', \\'work\\', \\'on\\', \\'last\\', \\'year.\\', \\'A\\', \\'customer\\', \\'approached\\', \\'me\\', \\'with\\', \\'a\\', \\'precious\\', \\'family\\', \\'heirloom\\', \\'-\\', \\'a\\', \\'diamond\\', \\'necklace\\', \\'that\\', \\'had\\', \\'been\\', \\'passed\\', \\'down\\', \\'through\\', \\'generations.\\', \\'Unfortunately,\\', \\'the\\', \\'necklace\\', \\'was\\', \\'in\\', \\'poor\\', \\'condition,\\', \\'with\\', \\'several\\', \\'loose\\', \\'diamonds\\', \\'and\\', \\'a\\', \\'broken\\', \\'clasp.\\', \\'The\\', \\'customer\\', \\'wanted\\', \\'me\\', \\'to\\', \\'restore\\', \\'it\\', \\'to\\', \\'its\\', \\'former\\', \\'glory,\\', \\'but\\', \\'it\\', \\'was\\', \\'clear\\', \\'that\\', \\'this\\', \\'would\\', \\'be\\', \\'no\\', \\'ordinary\\', \\'repair.\\', \\'Using\\', \\'my\\', \\'specialized\\', \\'tools\\', \\'and\\', \\'techniques,\\', \\'I\\', \\'began\\', \\'the\\', \\'delicate\\', \\'task\\', \\'of\\', \\'dismantling\\', \\'the\\', \\'necklace.\\', \\'Each\\', \\'diamond\\', \\'was\\', \\'carefully\\', \\'removed\\', \\'from\\', \\'its\\', \\'setting,\\', \\'and\\', \\'the\\', \\'damaged\\', \\'clasp\\', \\'was\\', \\'removed.\\', \\'Once\\', \\'the\\', \\'necklace\\', \\'was\\', \\'completely\\', \\'disassembled,\\', \\'I\\', \\'meticulously\\', \\'cleaned\\', \\'each\\', \\'diamond\\', \\'and\\', \\'inspected\\', \\'it\\', \\'for\\', \\'any\\', \\'damage.\\', \\'Fortunately,\\', \\'the\\', \\'diamonds\\', \\'were\\', \\'all\\', \\'in\\', \\'good\\', \\'condition,\\', \\'with\\', \\'no\\', \\'cracks\\', \\'or\\', \\'chips.\\', \\'The\\', \\'next\\', \\'step\\', \\'was\\', \\'to\\', \\'repair\\', \\'the\\', \\'broken\\', \\'clasp.\\', \\'I\\', \\'carefully\\', \\'soldered\\', \\'the\\', \\'broken\\', \\'pieces\\', \\'back\\', \\'together,\\', \\'ensuring\\', \\'that\\', \\'the\\', \\'clasp\\', \\'was\\', \\'sturdy\\', \\'and\\', \\'secure.\\', \\'Once\\', \\'the\\', \\'clasp\\', \\'was\\', \\'repaired,\\', \\'I\\', \\'began\\', \\'the\\', \\'process\\', \\'of\\', \\'reassembling\\', \\'the\\', \\'necklace.\\', \\'Each\\', \\'diamond\\', \\'was\\', \\'carefully\\', \\'placed\\', \\'back\\', \\'into\\', \\'its\\', \\'setting,\\', \\'and\\', \\'the\\', \\'necklace\\', \\'was\\', \\'polished\\', \\'until\\', \\'it\\', \\'sparkled\\', \\'like\\', \\'new.\\', \\'When\\', \\'I\\', \\'presented\\', \\'the\\', \\'restored\\', \\'necklace\\', \\'to\\', \\'the\\', \\'customer,\\', \\'they\\', \\'were\\', \\'overjoyed.\\', \\'They\\', \"couldn\\'t\", \\'believe\\', \\'that\\', \\'I\\', \\'had\\', \\'been\\', \\'able\\', \\'to\\', \\'bring\\', \\'their\\', \\'family\\', \\'heirloom\\', \\'back\\', \\'to\\', \\'life.\\', \\'The\\', \\'necklace\\', \\'looked\\', \\'as\\', \\'beautiful\\', \\'as\\', \\'it\\', \\'had\\', \\'when\\', \\'it\\', \\'was\\', \\'first\\', \\'created,\\', \\'and\\', \\'the\\', \\'customer\\', \\'was\\', \\'thrilled\\', \\'to\\', \\'have\\', \\'it\\', \\'back\\', \\'in\\', \\'their\\', \\'possession.\\', \\'If\\', \\'you\\', \\'have\\', \\'a\\', \\'project\\', \\'that\\', \\'you\\', \\'would\\', \\'like\\', \\'to\\', \\'discuss,\\', \\'please\\', \\'feel\\', \\'free\\', \\'to\\', \\'contact\\', \\'me\\', \\'by\\', \\'phone\\', \\'at\\', \\'(95)\\', \\'94215-7906\\', \\'or\\', \\'by\\', \\'email\\', \\'at\\', \\'aaliyah.popova4783@aol.edu.\\', \\'I\\', \\'look\\', \\'forward\\', \\'to\\', \\'hearing\\', \\'from\\', \\'you!\\', \\'P.S.:\\', \\'When\\', \"I\\'m\", \\'not\\', \\'creating\\', \\'beautiful\\', \\'jewelry,\\', \\'I\\', \\'enjoy\\', \\'spending\\', \\'time\\', \\'podcasting.\\', \\'I\\', \\'love\\', \\'sharing\\', \\'my\\', \\'knowledge\\', \\'about\\', \\'jewelry\\', \\'and\\', \\'connecting\\', \\'with\\', \\'other\\', \\'people\\', \\'who\\', \\'are\\', \\'passionate\\', \\'about\\', \\'this\\', \\'art\\', \\'form.\\', \\'I\\', \\'also\\', \\'enjoy\\', \\'spending\\', \\'time\\', \\'with\\', \\'my\\', \\'family\\', \\'and\\', \\'exploring\\', \\'new\\', \\'places.\\', \\'If\\', \\'you\\', \\'would\\', \\'like\\', \\'to\\', \\'learn\\', \\'more\\', \\'about\\', \\'me,\\', \\'please\\', \\'feel\\', \\'free\\', \\'to\\', \\'visit\\', \\'my\\', \\'website\\', \\'at\\', \\'[website\\', \\'address]\\', \\'or\\', \\'visit\\', \\'me\\', \\'at\\', \\'my\\', \\'studio\\', \\'located\\', \\'at\\', \\'97\\', \\'Lincoln\\', \\'Street.\\']', 'trailing_whitespace': '[True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False]', 'labels': \"['O', 'O', 'O', 'B-NAME_STUDENT', 'I-NAME_STUDENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PHONE_NUM', 'I-PHONE_NUM', 'O', 'O', 'O', 'O', 'B-EMAIL', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STREET_ADDRESS', 'I-STREET_ADDRESS', 'I-STREET_ADDRESS']\", 'prompt': '\\n    Aaliyah Popova is a jeweler with 13 years of experience. Write a detailed example in first person of a job-related project he/her did in the past. Add the following information about him/her randomly inside the text: name is Aaliyah Popova, phone number is (95) 94215-7906, email is aaliyah.popova4783@aol.edu, hobby is Podcasting, address is 97 Lincoln Street.\\n    ', 'prompt_id': 1, 'name': 'Aaliyah Popova', 'email': 'aaliyah.popova4783@aol.edu', 'phone': '(95) 94215-7906', 'job': 'jeweler', 'address': '97 Lincoln Street', 'username': nan, 'url': nan, 'hobby': 'Podcasting', 'len': 363}, {'document': '5ec717a9-17ee-48cd-9d76-30ae256c9354', 'text': \"My name is Konstantin Becker, and I'm a developer with two years of experience. I recently worked on a project where we built a new customer portal for our company. The goal was to create a more user-friendly and intuitive interface that would make it easier for customers to manage their accounts and access information. We started by gathering requirements from our customers and conducting user research to understand their needs and pain points. We then designed a new user interface that was both visually appealing and easy to use. We also implemented a number of new features, such as the ability for customers to view their account history, track their orders, and submit support tickets online. The project was a success, and our customers were very happy with the new portal. They found it to be much easier to use than the old one, and they appreciated the new features. We saw a significant increase in customer satisfaction and engagement as a result of the new portal. Throughout the project, I was responsible for developing the back-end code for the portal. I also worked closely with the design team to ensure that the portal was visually appealing and user-friendly. I'm proud of the work that I did on this project, and I'm confident that it will continue to benefit our customers for years to come. If you would like to contact me, my email address is konstantin.becker@gmail.com and my phone number is 0475 4429797. I live at 826 Webster Street. Quilting is my hobby.\", 'tokens': '[\\'My\\', \\'name\\', \\'is\\', \\'Konstantin\\', \\'Becker,\\', \\'and\\', \"I\\'m\", \\'a\\', \\'developer\\', \\'with\\', \\'two\\', \\'years\\', \\'of\\', \\'experience.\\', \\'I\\', \\'recently\\', \\'worked\\', \\'on\\', \\'a\\', \\'project\\', \\'where\\', \\'we\\', \\'built\\', \\'a\\', \\'new\\', \\'customer\\', \\'portal\\', \\'for\\', \\'our\\', \\'company.\\', \\'The\\', \\'goal\\', \\'was\\', \\'to\\', \\'create\\', \\'a\\', \\'more\\', \\'user-friendly\\', \\'and\\', \\'intuitive\\', \\'interface\\', \\'that\\', \\'would\\', \\'make\\', \\'it\\', \\'easier\\', \\'for\\', \\'customers\\', \\'to\\', \\'manage\\', \\'their\\', \\'accounts\\', \\'and\\', \\'access\\', \\'information.\\', \\'We\\', \\'started\\', \\'by\\', \\'gathering\\', \\'requirements\\', \\'from\\', \\'our\\', \\'customers\\', \\'and\\', \\'conducting\\', \\'user\\', \\'research\\', \\'to\\', \\'understand\\', \\'their\\', \\'needs\\', \\'and\\', \\'pain\\', \\'points.\\', \\'We\\', \\'then\\', \\'designed\\', \\'a\\', \\'new\\', \\'user\\', \\'interface\\', \\'that\\', \\'was\\', \\'both\\', \\'visually\\', \\'appealing\\', \\'and\\', \\'easy\\', \\'to\\', \\'use.\\', \\'We\\', \\'also\\', \\'implemented\\', \\'a\\', \\'number\\', \\'of\\', \\'new\\', \\'features,\\', \\'such\\', \\'as\\', \\'the\\', \\'ability\\', \\'for\\', \\'customers\\', \\'to\\', \\'view\\', \\'their\\', \\'account\\', \\'history,\\', \\'track\\', \\'their\\', \\'orders,\\', \\'and\\', \\'submit\\', \\'support\\', \\'tickets\\', \\'online.\\', \\'The\\', \\'project\\', \\'was\\', \\'a\\', \\'success,\\', \\'and\\', \\'our\\', \\'customers\\', \\'were\\', \\'very\\', \\'happy\\', \\'with\\', \\'the\\', \\'new\\', \\'portal.\\', \\'They\\', \\'found\\', \\'it\\', \\'to\\', \\'be\\', \\'much\\', \\'easier\\', \\'to\\', \\'use\\', \\'than\\', \\'the\\', \\'old\\', \\'one,\\', \\'and\\', \\'they\\', \\'appreciated\\', \\'the\\', \\'new\\', \\'features.\\', \\'We\\', \\'saw\\', \\'a\\', \\'significant\\', \\'increase\\', \\'in\\', \\'customer\\', \\'satisfaction\\', \\'and\\', \\'engagement\\', \\'as\\', \\'a\\', \\'result\\', \\'of\\', \\'the\\', \\'new\\', \\'portal.\\', \\'Throughout\\', \\'the\\', \\'project,\\', \\'I\\', \\'was\\', \\'responsible\\', \\'for\\', \\'developing\\', \\'the\\', \\'back-end\\', \\'code\\', \\'for\\', \\'the\\', \\'portal.\\', \\'I\\', \\'also\\', \\'worked\\', \\'closely\\', \\'with\\', \\'the\\', \\'design\\', \\'team\\', \\'to\\', \\'ensure\\', \\'that\\', \\'the\\', \\'portal\\', \\'was\\', \\'visually\\', \\'appealing\\', \\'and\\', \\'user-friendly.\\', \"I\\'m\", \\'proud\\', \\'of\\', \\'the\\', \\'work\\', \\'that\\', \\'I\\', \\'did\\', \\'on\\', \\'this\\', \\'project,\\', \\'and\\', \"I\\'m\", \\'confident\\', \\'that\\', \\'it\\', \\'will\\', \\'continue\\', \\'to\\', \\'benefit\\', \\'our\\', \\'customers\\', \\'for\\', \\'years\\', \\'to\\', \\'come.\\', \\'If\\', \\'you\\', \\'would\\', \\'like\\', \\'to\\', \\'contact\\', \\'me,\\', \\'my\\', \\'email\\', \\'address\\', \\'is\\', \\'konstantin.becker@gmail.com\\', \\'and\\', \\'my\\', \\'phone\\', \\'number\\', \\'is\\', \\'0475\\', \\'4429797.\\', \\'I\\', \\'live\\', \\'at\\', \\'826\\', \\'Webster\\', \\'Street.\\', \\'Quilting\\', \\'is\\', \\'my\\', \\'hobby.\\']', 'trailing_whitespace': '[True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False]', 'labels': \"['O', 'O', 'O', 'B-NAME_STUDENT', 'I-NAME_STUDENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-EMAIL', 'O', 'O', 'O', 'O', 'O', 'B-PHONE_NUM', 'I-PHONE_NUM', 'O', 'O', 'O', 'B-STREET_ADDRESS', 'I-STREET_ADDRESS', 'I-STREET_ADDRESS', 'O', 'O', 'O', 'O']\", 'prompt': '\\n    Konstantin Becker is a developer with 2 years of experience. Write a detailed example in first person of a job-related project he/her did in the past. Add the following information about him/her randomly inside the text: name is Konstantin Becker, phone number is 0475 4429797, email is konstantin.becker8489@gmail.com, hobby is Quilting, address is 826 Webster Street.\\n    ', 'prompt_id': 1, 'name': 'Konstantin Becker', 'email': 'konstantin.becker@gmail.com', 'phone': '0475 4429797', 'job': 'developer', 'address': '826 Webster Street', 'username': nan, 'url': nan, 'hobby': 'Quilting', 'len': 255}]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Adjust this if your filename differs\n",
    "csv_path = \"./pii_dataset.csv\"\n",
    "print(\"Using:\", csv_path)\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "\n",
    "# Peek at a few rows\n",
    "print(df.head(2).to_dict(orient=\"records\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a928b14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token column candidates: ['tokens']\n",
      "Label column candidates: ['labels']\n",
      "Row 0 token count vs label count: 363 vs 363\n",
      "First 10 tokens: ['My', 'name', 'is', 'Aaliyah', 'Popova,', 'and', 'I', 'am', 'a', 'jeweler']\n",
      "First 10 BIO tags: ['O', 'O', 'O', 'B-NAME_STUDENT', 'I-NAME_STUDENT', 'O', 'O', 'O', 'O', 'O']\n",
      "Rows with length mismatch: 0 out of 4434\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# If you haven't already loaded df:\n",
    "# df = pd.read_csv(\"<your path>.csv\")\n",
    "\n",
    "# 1) Find likely columns for tokens and labels\n",
    "tok_col_candidates = [c for c in df.columns if c.lower() in [\"tokens\",\"words\",\"tokens_list\",\"tokens_str\"]]\n",
    "lab_col_candidates = [c for c in df.columns if c.lower() in [\"labels\",\"ner_tags\",\"tags\"]]\n",
    "\n",
    "print(\"Token column candidates:\", tok_col_candidates)\n",
    "print(\"Label column candidates:\", lab_col_candidates)\n",
    "\n",
    "TOK_COL = tok_col_candidates[0]   # adjust if needed\n",
    "LAB_COL = lab_col_candidates[0]\n",
    "\n",
    "# 2) Ensure labels are a Python list (some CSVs save them as strings)\n",
    "def to_list(x):\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            return ast.literal_eval(x)\n",
    "        except Exception:\n",
    "            return x.split()  # last resort: space-separated\n",
    "    return list(x)\n",
    "\n",
    "df[\"TOKENS\"] = df[TOK_COL].apply(to_list)\n",
    "df[\"BIO\"]    = df[LAB_COL].apply(to_list)\n",
    "\n",
    "# 3) Check alignment and peek\n",
    "i = 0  # try a few different rows if needed\n",
    "print(\"Row\", i, \"token count vs label count:\",\n",
    "      len(df.loc[i, \"TOKENS\"]), \"vs\", len(df.loc[i, \"BIO\"]))\n",
    "\n",
    "print(\"First 10 tokens:\", df.loc[i, \"TOKENS\"][:10])\n",
    "print(\"First 10 BIO tags:\", df.loc[i, \"BIO\"][:10])\n",
    "\n",
    "# 4) How many rows mismatch?\n",
    "mismatch = (df[\"TOKENS\"].str.len() != df[\"BIO\"].str.len()).sum()\n",
    "print(\"Rows with length mismatch:\", mismatch, \"out of\", len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91bc6386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatches after mapping: 0\n",
      "BIO6 label distribution (top 15): [('O', 1333514), ('B-NAME', 11104), ('I-ADDRESS', 8577), ('I-NAME', 5667), ('B-EMAIL', 3794), ('B-ADDRESS', 3543), ('I-PHONE', 3389), ('B-PHONE', 2419), ('B-USERNAME', 718), ('B-URL_PERSONAL', 620)]\n",
      "[('My', 'O', 'O'), ('name', 'O', 'O'), ('is', 'O', 'O'), ('Aaliyah', 'B-NAME_STUDENT', 'B-NAME'), ('Popova,', 'I-NAME_STUDENT', 'I-NAME'), ('and', 'O', 'O'), ('I', 'O', 'O'), ('am', 'O', 'O'), ('a', 'O', 'O'), ('jeweler', 'O', 'O'), ('with', 'O', 'O'), ('13', 'O', 'O'), ('years', 'O', 'O'), ('of', 'O', 'O'), ('experience.', 'O', 'O'), ('I', 'O', 'O'), ('remember', 'O', 'O'), ('a', 'O', 'O'), ('very', 'O', 'O'), ('unique', 'O', 'O')]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# 1) Raw -> canonical mapping (now includes USERNAME, URL_PERSONAL)\n",
    "RAW2CANON = {\n",
    "    # Names\n",
    "    \"NAME_STUDENT\": \"NAME\", \"NAME\": \"NAME\", \"PERSON\": \"NAME\",\n",
    "    # Emails\n",
    "    \"EMAIL\": \"EMAIL\", \"EMAIL_ADDRESS\": \"EMAIL\",\n",
    "    # Phones\n",
    "    \"PHONE_NUM\": \"PHONE\", \"PHONE\": \"PHONE\", \"PHONE_NUMBER\": \"PHONE\",\n",
    "    # Addresses\n",
    "    \"STREET_ADDRESS\": \"ADDRESS\", \"ADDRESS\": \"ADDRESS\",\n",
    "    # Extras you asked to keep\n",
    "    \"USERNAME\": \"USERNAME\",\n",
    "    \"URL_PERSONAL\": \"URL_PERSONAL\",\n",
    "}\n",
    "\n",
    "TARGET_ENTS = {\"NAME\",\"EMAIL\",\"PHONE\",\"ADDRESS\",\"USERNAME\",\"URL_PERSONAL\"}\n",
    "\n",
    "def map_bio_tag(tag: str) -> str:\n",
    "    if tag == \"O\":\n",
    "        return \"O\"\n",
    "    if \"-\" not in tag:  # unexpected form\n",
    "        return \"O\"\n",
    "    prefix, raw = tag.split(\"-\", 1)  # e.g., 'B', 'NAME_STUDENT'\n",
    "    canon = RAW2CANON.get(raw)\n",
    "    if canon in TARGET_ENTS:\n",
    "        return f\"{prefix}-{canon}\"\n",
    "    return \"O\"\n",
    "\n",
    "# 2) Apply mapping\n",
    "df[\"BIO6\"] = df[\"BIO\"].apply(lambda seq: [map_bio_tag(t) for t in seq])\n",
    "\n",
    "# 3) Sanity checks\n",
    "mismatch_after = (df[\"TOKENS\"].str.len() != df[\"BIO6\"].str.len()).sum()\n",
    "print(\"Mismatches after mapping:\", mismatch_after)\n",
    "\n",
    "bio6_types = Counter()\n",
    "for tags in df[\"BIO6\"]:\n",
    "    bio6_types.update(tags)\n",
    "print(\"BIO6 label distribution (top 15):\", bio6_types.most_common(15))\n",
    "\n",
    "# 4) Quick peek\n",
    "i = 0\n",
    "print(list(zip(df.loc[i,\"TOKENS\"][:20], df.loc[i,\"BIO\"][:20], df.loc[i,\"BIO6\"][:20])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40780644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label vocab size: 13\n",
      "Unknown tags (should be empty): []\n",
      "Sample mapping: {'O': 0, 'B-NAME': 1, 'I-NAME': 2, 'B-EMAIL': 3, 'I-EMAIL': 4, 'B-PHONE': 5, 'I-PHONE': 6, 'B-ADDRESS': 7}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "ENTITIES = [\"NAME\",\"EMAIL\",\"PHONE\",\"ADDRESS\",\"USERNAME\",\"URL_PERSONAL\"]\n",
    "BIO_LABELS = [\"O\"] + [f\"{p}-{e}\" for e in ENTITIES for p in [\"B\",\"I\"]]\n",
    "label2id = {l:i for i,l in enumerate(BIO_LABELS)}\n",
    "id2label = {i:l for l,i in label2id.items()}\n",
    "\n",
    "# Verify: any tags outside this set?\n",
    "seen = Counter(t for row in df[\"BIO6\"] for t in row)\n",
    "unknown = [t for t in seen if t not in BIO_LABELS]\n",
    "print(\"Label vocab size:\", len(BIO_LABELS))\n",
    "print(\"Unknown tags (should be empty):\", unknown[:10])\n",
    "print(\"Sample mapping:\", {k:label2id[k] for k in BIO_LABELS[:8]})\n",
    "#“Sample mapping” is just a peek at the dictionary that converts your human-readable BIO tags into the integer IDs the model actually trains on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5a98a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'tags'],\n",
      "        num_rows: 3990\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'tags'],\n",
      "        num_rows: 444\n",
      "    })\n",
      "})\n",
      "Train rows: 3990 | Val rows: 444\n",
      "One row example: dict_keys(['tokens', 'tags'])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "ds_all = Dataset.from_pandas(\n",
    "    df[[\"TOKENS\",\"BIO6\"]].rename(columns={\"TOKENS\":\"tokens\",\"BIO6\":\"tags\"}),\n",
    "    preserve_index=False\n",
    ")\n",
    "\n",
    "# simple random split (90/10)\n",
    "N = len(ds_all)\n",
    "idx = np.arange(N)\n",
    "rng = np.random.default_rng(42)\n",
    "rng.shuffle(idx)\n",
    "cut = int(0.9*N)\n",
    "train_idx, val_idx = idx[:cut], idx[cut:]\n",
    "\n",
    "ds = DatasetDict({\n",
    "    \"train\": ds_all.select(train_idx.tolist()),\n",
    "    \"validation\": ds_all.select(val_idx.tolist()),\n",
    "})\n",
    "\n",
    "print(ds)\n",
    "print(\"Train rows:\", ds[\"train\"].num_rows, \"| Val rows:\", ds[\"validation\"].num_rows)\n",
    "print(\"One row example:\", ds[\"train\"][0].keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6335b49d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1cbe8d1718f4ca68232d792e225a602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing and aligning labels:   0%|          | 0/3990 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d37aa67d2164b3bb601c85d72e86d2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing and aligning labels:   0%|          | 0/444 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 3990\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 444\n",
      "    })\n",
      "})\n",
      "Keys in encoded batch: dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "len(input_ids) vs len(labels): 475 475\n",
      "Active labels in example row: 347\n",
      "First 20 labels (ids): [-100, 0, 0, 0, 0, 0, 0, 0, 0, -100, 1, 2, -100, 0, 0, 0, 0, 0, -100, 0]\n",
      "First 20 label names: ['PAD', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'PAD', 'B-NAME', 'I-NAME', 'PAD', 'O', 'O', 'O', 'O', 'O', 'PAD', 'O']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
    "\n",
    "def encode_batch(batch):\n",
    "    # Tokenize list-of-words with alignment info\n",
    "    enc = tokenizer(\n",
    "        batch[\"tokens\"],\n",
    "        is_split_into_words=True,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        max_length=512,\n",
    "    )\n",
    "    all_labels = []\n",
    "    for i, tags in enumerate(batch[\"tags\"]):\n",
    "        word_ids = enc.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        prev_wid = None\n",
    "        for wid in word_ids:\n",
    "            if wid is None:\n",
    "                label_ids.append(-100)            # special tokens\n",
    "            elif wid != prev_wid:\n",
    "                label_ids.append(label2id[tags[wid]])  # first subword of this word\n",
    "            else:\n",
    "                label_ids.append(-100)            # subsequent subwords\n",
    "            prev_wid = wid\n",
    "        all_labels.append(label_ids)\n",
    "    enc[\"labels\"] = all_labels\n",
    "    return enc\n",
    "\n",
    "encoded = ds.map(\n",
    "    encode_batch,\n",
    "    batched=True,\n",
    "    remove_columns=[\"tokens\",\"tags\"],\n",
    "    desc=\"Tokenizing and aligning labels\",\n",
    ")\n",
    "\n",
    "print(encoded)\n",
    "row = encoded[\"train\"][0]\n",
    "print(\"Keys in encoded batch:\", row.keys())\n",
    "print(\"len(input_ids) vs len(labels):\", len(row[\"input_ids\"]), len(row[\"labels\"]))\n",
    "# How many labels are active (i.e., not -100) in this example?\n",
    "active = sum(1 for x in row[\"labels\"] if x != -100)\n",
    "print(\"Active labels in example row:\", active)\n",
    "print(\"First 20 labels (ids):\", row[\"labels\"][:20])\n",
    "print(\"First 20 label names:\", [id2label[i] if i!=-100 else \"PAD\" for i in row[\"labels\"][:20]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ffab78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertForTokenClassification loaded with 13 labels.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoConfig\n",
    "\n",
    "MODEL_NAME = \"distilbert-base-cased\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(BIO_LABELS),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(MODEL_NAME, config=config)\n",
    "print(type(model).__name__, \"loaded with\", config.num_labels, \"labels.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2fe93a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Winston\\AppData\\Local\\Temp\\ipykernel_42720\\893606672.py:52: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForTokenClassification, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    \"\"\"\n",
    "    p.predictions: (batch, seq_len, num_labels)\n",
    "    p.label_ids:   (batch, seq_len) with -100 for ignored positions\n",
    "    \"\"\"\n",
    "    preds = np.argmax(p.predictions, axis=2)\n",
    "    true_preds, true_labels = [], []\n",
    "    for pred, lab in zip(preds, p.label_ids):\n",
    "        # keep only positions where label != -100\n",
    "        keep = lab != -100\n",
    "        pred_ids = pred[keep]\n",
    "        lab_ids  = lab[keep]\n",
    "        true_preds.append([id2label[int(i)] for i in pred_ids])\n",
    "        true_labels.append([id2label[int(i)] for i in lab_ids])\n",
    "\n",
    "    results = seqeval.compute(predictions=true_preds, references=true_labels, zero_division=0)\n",
    "    # Flatten to a friendly dict: overall + per-entity F1\n",
    "    out = {\n",
    "        \"overall_precision\": results[\"overall_precision\"],\n",
    "        \"overall_recall\": results[\"overall_recall\"],\n",
    "        \"overall_f1\": results[\"overall_f1\"],\n",
    "    }\n",
    "    for ent, stats in results.items():\n",
    "        if isinstance(stats, dict) and \"f1\" in stats:\n",
    "            out[f\"f1_{ent}\"] = stats[\"f1\"]\n",
    "    return out\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"pii-ner-distilbert\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"overall_f1\",\n",
    "    report_to=\"none\",\n",
    "    logging_steps=50,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=encoded[\"train\"],\n",
    "    eval_dataset=encoded[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "print(\"Trainer ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2a2241",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Winston\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 21/750 11:03 < 7:04:21, 0.03 it/s, Epoch 0.08/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_out \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Training summary ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_out)\n",
      "File \u001b[1;32mc:\\Users\\Winston\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:2238\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2236\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2237\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2238\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   2239\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   2240\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[0;32m   2241\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[0;32m   2242\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[0;32m   2243\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Winston\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:2582\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2575\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2576\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2577\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2578\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2579\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2580\u001b[0m )\n\u001b[0;32m   2581\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2582\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[0;32m   2584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2585\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2586\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2587\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2588\u001b[0m ):\n\u001b[0;32m   2589\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2590\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\Winston\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:3796\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3793\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   3795\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 3796\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(model, inputs, num_items_in_batch\u001b[38;5;241m=\u001b[39mnum_items_in_batch)\n\u001b[0;32m   3798\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[0;32m   3799\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   3800\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   3802\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\Winston\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:3884\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3882\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[0;32m   3883\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[1;32m-> 3884\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m   3885\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   3886\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   3887\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Winston\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Winston\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Winston\\anaconda3\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:1129\u001b[0m, in \u001b[0;36mDistilBertForTokenClassification.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1124\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m \u001b[38;5;124;03m    Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\u001b[39;00m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1129\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistilbert(\n\u001b[0;32m   1130\u001b[0m     input_ids,\n\u001b[0;32m   1131\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   1132\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m   1133\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m   1134\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1135\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1136\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1137\u001b[0m )\n\u001b[0;32m   1139\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1141\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(sequence_output)\n",
      "File \u001b[1;32mc:\\Users\\Winston\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Winston\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Winston\\anaconda3\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:736\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_sdpa \u001b[38;5;129;01mand\u001b[39;00m head_mask_is_none \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m output_attentions:\n\u001b[0;32m    732\u001b[0m         attention_mask \u001b[38;5;241m=\u001b[39m _prepare_4d_attention_mask_for_sdpa(\n\u001b[0;32m    733\u001b[0m             attention_mask, embeddings\u001b[38;5;241m.\u001b[39mdtype, tgt_len\u001b[38;5;241m=\u001b[39minput_shape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    734\u001b[0m         )\n\u001b[1;32m--> 736\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(\n\u001b[0;32m    737\u001b[0m     x\u001b[38;5;241m=\u001b[39membeddings,\n\u001b[0;32m    738\u001b[0m     attn_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    739\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m    740\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    741\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m    742\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m    743\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Winston\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Winston\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Winston\\anaconda3\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:541\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[0;32m    539\u001b[0m     all_hidden_states \u001b[38;5;241m=\u001b[39m all_hidden_states \u001b[38;5;241m+\u001b[39m (hidden_state,)\n\u001b[1;32m--> 541\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[0;32m    542\u001b[0m     hidden_state,\n\u001b[0;32m    543\u001b[0m     attn_mask,\n\u001b[0;32m    544\u001b[0m     head_mask[i],\n\u001b[0;32m    545\u001b[0m     output_attentions,\n\u001b[0;32m    546\u001b[0m )\n\u001b[0;32m    548\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Users\\Winston\\anaconda3\\Lib\\site-packages\\transformers\\modeling_layers.py:93\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     90\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m---> 93\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Winston\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Winston\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Winston\\anaconda3\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:476\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[1;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;124;03mParameters:\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;124;03m    x: torch.tensor(bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m    torch.tensor(bs, seq_length, dim) The output of the transformer block contextualization.\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;66;03m# Self-Attention\u001b[39;00m\n\u001b[1;32m--> 476\u001b[0m sa_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(\n\u001b[0;32m    477\u001b[0m     query\u001b[38;5;241m=\u001b[39mx,\n\u001b[0;32m    478\u001b[0m     key\u001b[38;5;241m=\u001b[39mx,\n\u001b[0;32m    479\u001b[0m     value\u001b[38;5;241m=\u001b[39mx,\n\u001b[0;32m    480\u001b[0m     mask\u001b[38;5;241m=\u001b[39mattn_mask,\n\u001b[0;32m    481\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m    482\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    483\u001b[0m )\n\u001b[0;32m    484\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[0;32m    485\u001b[0m     sa_output, sa_weights \u001b[38;5;241m=\u001b[39m sa_output  \u001b[38;5;66;03m# (bs, seq_length, dim), (bs, n_heads, seq_length, seq_length)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Winston\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Winston\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Winston\\anaconda3\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:402\u001b[0m, in \u001b[0;36mDistilBertSdpaAttention.forward\u001b[1;34m(self, query, key, value, mask, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    399\u001b[0m     k \u001b[38;5;241m=\u001b[39m k\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    400\u001b[0m     v \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m--> 402\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mscaled_dot_product_attention(\n\u001b[0;32m    403\u001b[0m     q,\n\u001b[0;32m    404\u001b[0m     k,\n\u001b[0;32m    405\u001b[0m     v,\n\u001b[0;32m    406\u001b[0m     attn_mask\u001b[38;5;241m=\u001b[39mmask,\n\u001b[0;32m    407\u001b[0m     dropout_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout_prob \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m,\n\u001b[0;32m    408\u001b[0m     is_causal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    409\u001b[0m )\n\u001b[0;32m    411\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m unshape(attn_output)\n\u001b[0;32m    412\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_lin(attn_output)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_out = trainer.train()\n",
    "print(\"\\n=== Training summary ===\")\n",
    "print(train_out)\n",
    "\n",
    "eval_out = trainer.evaluate()\n",
    "print(\"\\n=== Eval metrics (validation) ===\")\n",
    "for k,v in sorted(eval_out.items()):\n",
    "    if k.startswith(\"eval_\"):\n",
    "        print(f\"{k}: {v:.4f}\" if isinstance(v, (int,float)) else f\"{k}: {v}\")\n",
    "\n",
    "# ---------- SAVE THE MODEL & TOKENIZER ----------\n",
    "SAVE_DIR = \"pii-ner-distilbert\"   # reuse output_dir or choose a new folder\n",
    "\n",
    "trainer.save_model(SAVE_DIR)      # writes pytorch_model.bin + config.json (with id2label/label2id)\n",
    "tokenizer.save_pretrained(SAVE_DIR)\n",
    "\n",
    "# quick confirmation\n",
    "print(\"\\nSaved files in\", SAVE_DIR, \":\\n\", os.listdir(SAVE_DIR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "506e6499",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96aecfbed36a48ada3f354886ef922d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenize (speed mode):   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5261098f92524a3184750ea94f4cc0ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenize (speed mode):   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Winston\\AppData\\Local\\Temp\\ipykernel_42720\\1479944800.py:100: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer ready (speed mode).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='47' max='47' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [47/47 05:42, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training summary (speed mode) ===\n",
      "TrainOutput(global_step=47, training_loss=0.23462857591344954, metrics={'train_runtime': 380.3809, 'train_samples_per_second': 3.943, 'train_steps_per_second': 0.124, 'total_flos': 49004657280000.0, 'train_loss': 0.23462857591344954, 'epoch': 1.0})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Winston\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Eval metrics (validation, speed mode) ===\n",
      "eval_f1_ADDRESS: 0.8056\n",
      "eval_f1_EMAIL: 0.0000\n",
      "eval_f1_NAME: 0.6796\n",
      "eval_f1_PHONE: 0.0000\n",
      "eval_f1_URL_PERSONAL: 0.0000\n",
      "eval_f1_USERNAME: 0.0000\n",
      "eval_loss: 0.0326\n",
      "eval_overall_f1: 0.6621\n",
      "eval_overall_precision: 0.6175\n",
      "eval_overall_recall: 0.7137\n",
      "eval_runtime: 50.4945\n",
      "eval_samples_per_second: 5.9410\n",
      "eval_steps_per_second: 0.1980\n",
      "\n",
      "Model saved to: pii-ner-fast\n"
     ]
    }
   ],
   "source": [
    "# --- SPEED MODE SWITCHES ---\n",
    "MAX_LEN = 128            # shorter sequences = much faster\n",
    "TRAIN_ROWS = 1500        # small train subset\n",
    "VAL_ROWS = 300           # small val subset\n",
    "EPOCHS = 1               # quick pass\n",
    "BATCH = 32               # try 24/32; lower if you hit RAM issues\n",
    "\n",
    "# 1) Small, fast backbone\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForTokenClassification\n",
    "MODEL_NAME = \"distilbert-base-cased\"  # tiny & quick\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "config = AutoConfig.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(BIO_LABELS),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "model = AutoModelForTokenClassification.from_pretrained(MODEL_NAME, config=config)\n",
    "\n",
    "# 2) Re-tokenize with shorter max_length and first-subword labeling\n",
    "def encode_batch(batch):\n",
    "    enc = tokenizer(\n",
    "        batch[\"tokens\"],\n",
    "        is_split_into_words=True,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        max_length=MAX_LEN,\n",
    "    )\n",
    "    all_labels = []\n",
    "    for i, tags in enumerate(batch[\"tags\"]):\n",
    "        word_ids = enc.word_ids(batch_index=i)\n",
    "        label_ids, prev_wid = [], None\n",
    "        for wid in word_ids:\n",
    "            if wid is None:\n",
    "                label_ids.append(-100)\n",
    "            elif wid != prev_wid:\n",
    "                label_ids.append(label2id[tags[wid]])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            prev_wid = wid\n",
    "        all_labels.append(label_ids)\n",
    "    enc[\"labels\"] = all_labels\n",
    "    return enc\n",
    "\n",
    "from datasets import DatasetDict\n",
    "encoded_fast = DatasetDict({\n",
    "    \"train\": ds[\"train\"].select(range(min(TRAIN_ROWS, ds[\"train\"].num_rows))),\n",
    "    \"validation\": ds[\"validation\"].select(range(min(VAL_ROWS, ds[\"validation\"].num_rows))),\n",
    "}).map(encode_batch, batched=True, remove_columns=[\"tokens\",\"tags\"], desc=\"Tokenize (speed mode)\")\n",
    "\n",
    "# 3) Compat-safe Trainer args (only pass what your version supports)\n",
    "from transformers import DataCollatorForTokenClassification, TrainingArguments, Trainer\n",
    "import inspect, numpy as np, evaluate\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=2)\n",
    "    true_preds, true_labels = [], []\n",
    "    for pred, lab in zip(preds, p.label_ids):\n",
    "        keep = lab != -100\n",
    "        true_preds.append([id2label[int(i)] for i in pred[keep]])\n",
    "        true_labels.append([id2label[int(i)] for i in lab[keep]])\n",
    "    res = seqeval.compute(predictions=true_preds, references=true_labels, zero_division=0)\n",
    "    out = {\n",
    "        \"overall_precision\": res.get(\"overall_precision\", 0.0),\n",
    "        \"overall_recall\": res.get(\"overall_recall\", 0.0),\n",
    "        \"overall_f1\": res.get(\"overall_f1\", 0.0),\n",
    "    }\n",
    "    for ent, st in res.items():\n",
    "        if isinstance(st, dict) and \"f1\" in st:\n",
    "            out[f\"f1_{ent}\"] = st[\"f1\"]\n",
    "    return out\n",
    "\n",
    "sig = inspect.signature(TrainingArguments.__init__)\n",
    "params = set(sig.parameters.keys())\n",
    "\n",
    "kwargs = dict(\n",
    "    output_dir=\"pii-ner-fast\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=BATCH,\n",
    "    per_device_eval_batch_size=BATCH,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=0.0,      # less overhead\n",
    "    logging_steps=100,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# add optional args only if your installed version supports them\n",
    "if \"dataloader_num_workers\" in params:\n",
    "    kwargs[\"dataloader_num_workers\"] = 2\n",
    "if \"report_to\" in params:\n",
    "    kwargs[\"report_to\"] = \"none\"\n",
    "# we skip eval/save strategies during training; we’ll eval once after\n",
    "\n",
    "args = TrainingArguments(**kwargs)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=encoded_fast[\"train\"],\n",
    "    eval_dataset=encoded_fast[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Trainer ready (speed mode).\")\n",
    "\n",
    "# 4) Train quick, then evaluate once\n",
    "train_out = trainer.train()\n",
    "print(\"\\n=== Training summary (speed mode) ===\")\n",
    "print(train_out)\n",
    "\n",
    "eval_out = trainer.evaluate()\n",
    "print(\"\\n=== Eval metrics (validation, speed mode) ===\")\n",
    "for k, v in sorted(eval_out.items()):\n",
    "    if k.startswith(\"eval_\"):\n",
    "        print(f\"{k}: {v:.4f}\" if isinstance(v, (int,float)) else f\"{k}: {v}\")\n",
    "\n",
    "# Save a checkpoint you can test with Step 8\n",
    "trainer.save_model(\"pii-ner-fast\")\n",
    "tokenizer.save_pretrained(\"pii-ner-fast\")\n",
    "print(\"\\nModel saved to: pii-ner-fast\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee54320a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tags: ['B-NAME', 'I-NAME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-ADDRESS', 'I-ADDRESS', 'I-ADDRESS', 'I-ADDRESS', 'I-ADDRESS', 'O', 'I-ADDRESS', 'I-ADDRESS', 'I-ADDRESS', 'I-ADDRESS', 'I-ADDRESS', 'I-ADDRESS', 'I-ADDRESS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-ADDRESS', 'I-ADDRESS', 'I-ADDRESS', 'I-ADDRESS', 'I-ADDRESS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "offs: [(0, 9), (10, 14), (15, 17), (18, 19), (20, 27), (28, 30), (31, 32), (32, 34), (34, 35), (36, 37), (38, 42), (43, 45), (46, 51), (52, 54), (55, 57), (57, 58), (59, 62), (62, 64), (65, 67), (67, 71), (72, 73), (73, 74), (75, 76), (76, 78), (78, 79), (79, 82), (82, 83), (84, 85), (86, 88), (89, 90), (91, 98), (99, 101), (102, 103), (103, 105), (105, 106), (107, 109), (110, 115), (116, 118), (119, 120), (120, 121), (121, 122), (122, 124), (124, 125), (125, 126), (126, 127), (127, 128), (128, 129), (129, 130), (130, 131), (131, 133), (133, 134), (134, 136), (136, 137)]\n",
      "\n",
      "IN : Catherine Kang is a student at NUS. I live at Block 60 Toa Payoh Lorong 4, #11-113. I am a student at NUS. My email is e0960493@u.nus.edu\n",
      "OUT: [NAME] is a student at NUS. I live at [ADDRESS]oh [ADDRESS]113. I am a student at NUS. My email is [EMAIL]\n",
      "SPN: [{'start': 0, 'end': 14, 'label': 'NAME'}, {'start': 46, 'end': 62, 'label': 'ADDRESS'}, {'start': 65, 'end': 79, 'label': 'ADDRESS'}, {'start': 119, 'end': 137, 'label': 'EMAIL'}]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "\n",
    "MODEL_DIR = \"./pii-ner-fast\"  # <- path to your saved model folder\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "mdl = AutoModelForTokenClassification.from_pretrained(MODEL_DIR)\n",
    "mdl.eval()\n",
    "\n",
    "def coalesce_same_label_spans(spans, text, max_gap_chars=2):\n",
    "    \"\"\"\n",
    "    Merge consecutive spans with the same label if the gap between them is tiny\n",
    "    (e.g., subword tail like 'oh' or '#11'), so '[ADDRESS]oh [ADDRESS]113' -> '[ADDRESS]'.\n",
    "    \"\"\"\n",
    "    if not spans:\n",
    "        return spans\n",
    "    spans = sorted(spans, key=lambda s: (s[\"start\"], s[\"end\"]))\n",
    "    merged = [spans[0]]\n",
    "\n",
    "    for s in spans[1:]:\n",
    "        prev = merged[-1]\n",
    "        if s[\"label\"] == prev[\"label\"]:\n",
    "            gap = text[prev[\"end\"]:s[\"start\"]]\n",
    "            # allow tiny tails: up to N alnum or -/#, with optional surrounding whitespace\n",
    "            if re.fullmatch(rf\"\\s*[-/#A-Za-z0-9]{{0,{max_gap_chars}}}\\s*\", gap):\n",
    "                # extend previous span to include gap + current span\n",
    "                prev[\"end\"] = s[\"end\"]\n",
    "                continue\n",
    "        merged.append(s)\n",
    "    return merged\n",
    "\n",
    "def predict_tags(text: str):\n",
    "    enc = tok(text, return_offsets_mapping=True, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        out = mdl(input_ids=enc[\"input_ids\"], attention_mask=enc[\"attention_mask\"])\n",
    "    pred_ids = out.logits.argmax(-1)[0].tolist()\n",
    "    tags = [mdl.config.id2label[int(i)] for i in pred_ids]\n",
    "\n",
    "    # drop special tokens (offsets == (0,0))\n",
    "    offsets = enc[\"offset_mapping\"][0].tolist()\n",
    "    toks, clean_tags, clean_offs = [], [], []\n",
    "    for (s,e), tag in zip(offsets, tags):\n",
    "        if s==0 and e==0:  # skip special tokens\n",
    "            continue\n",
    "        toks.append(text[s:e]); clean_tags.append(tag); clean_offs.append((s,e))\n",
    "    return toks, clean_tags, clean_offs\n",
    "\n",
    "\n",
    "# Merge BIO → character spans\n",
    "def bio_to_char_spans(offsets, tags):\n",
    "    spans, cur = [], None\n",
    "    for (s,e), tag in zip(offsets, tags):\n",
    "        if tag.startswith(\"B-\"):\n",
    "            if cur: spans.append(cur)\n",
    "            cur = {\"start\": s, \"end\": e, \"label\": tag.split(\"-\",1)[1]}\n",
    "        elif tag.startswith(\"I-\"):\n",
    "            ent = tag.split(\"-\",1)[1]\n",
    "            if cur and cur[\"label\"] == ent and s <= cur[\"end\"] + 1:\n",
    "                cur[\"end\"] = e\n",
    "            else:\n",
    "                cur = {\"start\": s, \"end\": e, \"label\": ent}\n",
    "        else:\n",
    "            if cur: spans.append(cur); cur = None\n",
    "    if cur: spans.append(cur)\n",
    "    return spans\n",
    "\n",
    "# High-precision regex for structured PII\n",
    "EMAIL_RE = re.compile(r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b\")\n",
    "PHONE_RE = re.compile(r\"\\b(?:\\+?\\d{1,3}[-.\\s]?)?(?:\\(?\\d{1,4}\\)?[-.\\s]?)?\\d{3,4}[-.\\s]?\\d{3,4}\\b\")\n",
    "URL_RE   = re.compile(r\"\\bhttps?://[^\\s]+\", re.I)\n",
    "HANDLE_RE= re.compile(r\"@\\w{1,32}\")\n",
    "\n",
    "def regex_spans(text):\n",
    "    out = []\n",
    "    for m in EMAIL_RE.finditer(text):\n",
    "        out.append({\"start\": m.start(), \"end\": m.end(), \"label\": \"EMAIL\"})\n",
    "    for m in PHONE_RE.finditer(text):\n",
    "        out.append({\"start\": m.start(), \"end\": m.end(), \"label\": \"PHONE\"})\n",
    "    for m in URL_RE.finditer(text):\n",
    "        out.append({\"start\": m.start(), \"end\": m.end(), \"label\": \"URL_PERSONAL\"})\n",
    "    for m in HANDLE_RE.finditer(text):\n",
    "        out.append({\"start\": m.start(), \"end\": m.end(), \"label\": \"USERNAME\"})\n",
    "    return out\n",
    "\n",
    "def ner_spans(text):\n",
    "    toks, tags, offs = predict_tags(text)\n",
    "    print(\"tags:\", tags)\n",
    "    print(\"offs:\", offs)\n",
    "    return bio_to_char_spans(offs, tags)\n",
    "\n",
    "def merge_spans(spans):\n",
    "    spans = sorted(spans, key=lambda s: (s[\"start\"], -(s[\"end\"]-s[\"start\"])))\n",
    "    merged = []\n",
    "    for s in spans:\n",
    "        if not merged or s[\"start\"] >= merged[-1][\"end\"]:\n",
    "            merged.append(s)\n",
    "        else:\n",
    "            if (s[\"end\"]-s[\"start\"]) > (merged[-1][\"end\"]-merged[-1][\"start\"]):\n",
    "                merged[-1] = s\n",
    "    return merged\n",
    "\n",
    "def redact(text, style=\"tags\"):\n",
    "    # regex for EMAIL/PHONE/URL/USERNAME + NER for NAME/ADDRESS (+ any extras the model finds)\n",
    "    r = regex_spans(text)\n",
    "    n = ner_spans(text)\n",
    "\n",
    "    keep = r[:]  # always keep structured regex hits\n",
    "    covered = {(s[\"start\"], s[\"end\"]) for s in r}\n",
    "    for s in n:\n",
    "        # Always include model-detected NAME/ADDRESS; include others if regex didn’t already catch\n",
    "        if s[\"label\"] in {\"NAME\",\"ADDRESS\"} or (s[\"start\"], s[\"end\"]) not in covered:\n",
    "            keep.append(s)\n",
    "\n",
    "    spans = merge_spans(keep)\n",
    "    spans = coalesce_same_label_spans(spans, text, max_gap_chars=2)\n",
    "\n",
    "    out, last = [], 0\n",
    "    for s in spans:\n",
    "        out.append(text[last:s[\"start\"]])\n",
    "        token = f\"[{s['label']}]\" if style==\"tags\" else \"█\"*(s[\"end\"]-s[\"start\"])\n",
    "        out.append(token)\n",
    "        last = s[\"end\"]\n",
    "    out.append(text[last:])\n",
    "    return \"\".join(out), spans\n",
    "\n",
    "# Try a few examples:\n",
    "samples = [\n",
    "    \"Catherine Kang is a student at NUS. I live at Block 60 Toa Payoh Lorong 4, #11-113. I am a student at NUS. My email is e0960493@u.nus.edu\"\n",
    "]\n",
    "for s in samples:\n",
    "    red, spans = redact(s, style=\"tags\")\n",
    "    print(\"\\nIN :\", s)\n",
    "    print(\"OUT:\", red)\n",
    "    print(\"SPN:\", spans)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
