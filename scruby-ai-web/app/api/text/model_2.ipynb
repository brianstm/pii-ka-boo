{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7c6bf37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.55.4\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ed10cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Winston\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:61: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: ./pii_dataset.csv\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './pii_dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m csv_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./pii_dataset.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing:\u001b[39m\u001b[38;5;124m\"\u001b[39m, csv_path)\n\u001b[1;32m----> 8\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(csv_path)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, df\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumns:\u001b[39m\u001b[38;5;124m\"\u001b[39m, df\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist())\n",
      "File \u001b[1;32mc:\\Users\\Winston\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\Winston\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\Winston\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32mc:\\Users\\Winston\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\Winston\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './pii_dataset.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Adjust this if your filename differs\n",
    "csv_path = \"./pii_dataset.csv\"\n",
    "print(\"Using:\", csv_path)\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "\n",
    "# Peek at a few rows\n",
    "print(df.head(2).to_dict(orient=\"records\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a928b14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token column candidates: ['tokens']\n",
      "Label column candidates: ['labels']\n",
      "Row 0 token count vs label count: 363 vs 363\n",
      "First 10 tokens: ['My', 'name', 'is', 'Aaliyah', 'Popova,', 'and', 'I', 'am', 'a', 'jeweler']\n",
      "First 10 BIO tags: ['O', 'O', 'O', 'B-NAME_STUDENT', 'I-NAME_STUDENT', 'O', 'O', 'O', 'O', 'O']\n",
      "Rows with length mismatch: 0 out of 4434\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# If you haven't already loaded df:\n",
    "# df = pd.read_csv(\"<your path>.csv\")\n",
    "\n",
    "# 1) Find likely columns for tokens and labels\n",
    "tok_col_candidates = [c for c in df.columns if c.lower() in [\"tokens\",\"words\",\"tokens_list\",\"tokens_str\"]]\n",
    "lab_col_candidates = [c for c in df.columns if c.lower() in [\"labels\",\"ner_tags\",\"tags\"]]\n",
    "\n",
    "print(\"Token column candidates:\", tok_col_candidates)\n",
    "print(\"Label column candidates:\", lab_col_candidates)\n",
    "\n",
    "TOK_COL = tok_col_candidates[0]   # adjust if needed\n",
    "LAB_COL = lab_col_candidates[0]\n",
    "\n",
    "# 2) Ensure labels are a Python list (some CSVs save them as strings)\n",
    "def to_list(x):\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            return ast.literal_eval(x)\n",
    "        except Exception:\n",
    "            return x.split()  # last resort: space-separated\n",
    "    return list(x)\n",
    "\n",
    "df[\"TOKENS\"] = df[TOK_COL].apply(to_list)\n",
    "df[\"BIO\"]    = df[LAB_COL].apply(to_list)\n",
    "\n",
    "# 3) Check alignment and peek\n",
    "i = 0  # try a few different rows if needed\n",
    "print(\"Row\", i, \"token count vs label count:\",\n",
    "      len(df.loc[i, \"TOKENS\"]), \"vs\", len(df.loc[i, \"BIO\"]))\n",
    "\n",
    "print(\"First 10 tokens:\", df.loc[i, \"TOKENS\"][:10])\n",
    "print(\"First 10 BIO tags:\", df.loc[i, \"BIO\"][:10])\n",
    "\n",
    "# 4) How many rows mismatch?\n",
    "mismatch = (df[\"TOKENS\"].str.len() != df[\"BIO\"].str.len()).sum()\n",
    "print(\"Rows with length mismatch:\", mismatch, \"out of\", len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bc6386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatches after mapping: 0\n",
      "BIO6 label distribution (top 15): [('O', 1333514), ('B-NAME', 11104), ('I-ADDRESS', 8577), ('I-NAME', 5667), ('B-EMAIL', 3794), ('B-ADDRESS', 3543), ('I-PHONE', 3389), ('B-PHONE', 2419), ('B-USERNAME', 718), ('B-URL_PERSONAL', 620)]\n",
      "[('My', 'O', 'O'), ('name', 'O', 'O'), ('is', 'O', 'O'), ('Aaliyah', 'B-NAME_STUDENT', 'B-NAME'), ('Popova,', 'I-NAME_STUDENT', 'I-NAME'), ('and', 'O', 'O'), ('I', 'O', 'O'), ('am', 'O', 'O'), ('a', 'O', 'O'), ('jeweler', 'O', 'O'), ('with', 'O', 'O'), ('13', 'O', 'O'), ('years', 'O', 'O'), ('of', 'O', 'O'), ('experience.', 'O', 'O'), ('I', 'O', 'O'), ('remember', 'O', 'O'), ('a', 'O', 'O'), ('very', 'O', 'O'), ('unique', 'O', 'O')]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# 1) Raw -> canonical mapping (now includes USERNAME, URL_PERSONAL)\n",
    "RAW2CANON = {\n",
    "    # Names\n",
    "    \"NAME_STUDENT\": \"NAME\", \"NAME\": \"NAME\", \"PERSON\": \"NAME\",\n",
    "    # Emails\n",
    "    \"EMAIL\": \"EMAIL\", \"EMAIL_ADDRESS\": \"EMAIL\",\n",
    "    # Phones\n",
    "    \"PHONE_NUM\": \"PHONE\", \"PHONE\": \"PHONE\", \"PHONE_NUMBER\": \"PHONE\",\n",
    "    # Addresses\n",
    "    \"STREET_ADDRESS\": \"ADDRESS\", \"ADDRESS\": \"ADDRESS\",\n",
    "    # Extras you asked to keep\n",
    "    \"USERNAME\": \"USERNAME\",\n",
    "    \"URL_PERSONAL\": \"URL_PERSONAL\",\n",
    "}\n",
    "\n",
    "TARGET_ENTS = {\"NAME\",\"EMAIL\",\"PHONE\",\"ADDRESS\",\"USERNAME\",\"URL_PERSONAL\"}\n",
    "\n",
    "def map_bio_tag(tag: str) -> str:\n",
    "    if tag == \"O\":\n",
    "        return \"O\"\n",
    "    if \"-\" not in tag:  # unexpected form\n",
    "        return \"O\"\n",
    "    prefix, raw = tag.split(\"-\", 1)  # e.g., 'B', 'NAME_STUDENT'\n",
    "    canon = RAW2CANON.get(raw)\n",
    "    if canon in TARGET_ENTS:\n",
    "        return f\"{prefix}-{canon}\"\n",
    "    return \"O\"\n",
    "\n",
    "# 2) Apply mapping\n",
    "df[\"BIO6\"] = df[\"BIO\"].apply(lambda seq: [map_bio_tag(t) for t in seq])\n",
    "\n",
    "# 3) Sanity checks\n",
    "mismatch_after = (df[\"TOKENS\"].str.len() != df[\"BIO6\"].str.len()).sum()\n",
    "print(\"Mismatches after mapping:\", mismatch_after)\n",
    "\n",
    "bio6_types = Counter()\n",
    "for tags in df[\"BIO6\"]:\n",
    "    bio6_types.update(tags)\n",
    "print(\"BIO6 label distribution (top 15):\", bio6_types.most_common(15))\n",
    "\n",
    "# 4) Quick peek\n",
    "i = 0\n",
    "print(list(zip(df.loc[i,\"TOKENS\"][:20], df.loc[i,\"BIO\"][:20], df.loc[i,\"BIO6\"][:20])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40780644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label vocab size: 13\n",
      "Unknown tags (should be empty): []\n",
      "Sample mapping: {'O': 0, 'B-NAME': 1, 'I-NAME': 2, 'B-EMAIL': 3, 'I-EMAIL': 4, 'B-PHONE': 5, 'I-PHONE': 6, 'B-ADDRESS': 7}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "ENTITIES = [\"NAME\",\"EMAIL\",\"PHONE\",\"ADDRESS\",\"USERNAME\",\"URL_PERSONAL\"]\n",
    "BIO_LABELS = [\"O\"] + [f\"{p}-{e}\" for e in ENTITIES for p in [\"B\",\"I\"]]\n",
    "label2id = {l:i for i,l in enumerate(BIO_LABELS)}\n",
    "id2label = {i:l for l,i in label2id.items()}\n",
    "\n",
    "# Verify: any tags outside this set?\n",
    "seen = Counter(t for row in df[\"BIO6\"] for t in row)\n",
    "unknown = [t for t in seen if t not in BIO_LABELS]\n",
    "print(\"Label vocab size:\", len(BIO_LABELS))\n",
    "print(\"Unknown tags (should be empty):\", unknown[:10])\n",
    "print(\"Sample mapping:\", {k:label2id[k] for k in BIO_LABELS[:8]})\n",
    "#“Sample mapping” is just a peek at the dictionary that converts your human-readable BIO tags into the integer IDs the model actually trains on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a98a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'tags'],\n",
      "        num_rows: 3990\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'tags'],\n",
      "        num_rows: 444\n",
      "    })\n",
      "})\n",
      "Train rows: 3990 | Val rows: 444\n",
      "One row example: dict_keys(['tokens', 'tags'])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "ds_all = Dataset.from_pandas(\n",
    "    df[[\"TOKENS\",\"BIO6\"]].rename(columns={\"TOKENS\":\"tokens\",\"BIO6\":\"tags\"}),\n",
    "    preserve_index=False\n",
    ")\n",
    "\n",
    "# simple random split (90/10)\n",
    "N = len(ds_all)\n",
    "idx = np.arange(N)\n",
    "rng = np.random.default_rng(42)\n",
    "rng.shuffle(idx)\n",
    "cut = int(0.9*N)\n",
    "train_idx, val_idx = idx[:cut], idx[cut:]\n",
    "\n",
    "ds = DatasetDict({\n",
    "    \"train\": ds_all.select(train_idx.tolist()),\n",
    "    \"validation\": ds_all.select(val_idx.tolist()),\n",
    "})\n",
    "\n",
    "print(ds)\n",
    "print(\"Train rows:\", ds[\"train\"].num_rows, \"| Val rows:\", ds[\"validation\"].num_rows)\n",
    "print(\"One row example:\", ds[\"train\"][0].keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6335b49d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1cbe8d1718f4ca68232d792e225a602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing and aligning labels:   0%|          | 0/3990 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d37aa67d2164b3bb601c85d72e86d2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing and aligning labels:   0%|          | 0/444 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 3990\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 444\n",
      "    })\n",
      "})\n",
      "Keys in encoded batch: dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "len(input_ids) vs len(labels): 475 475\n",
      "Active labels in example row: 347\n",
      "First 20 labels (ids): [-100, 0, 0, 0, 0, 0, 0, 0, 0, -100, 1, 2, -100, 0, 0, 0, 0, 0, -100, 0]\n",
      "First 20 label names: ['PAD', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'PAD', 'B-NAME', 'I-NAME', 'PAD', 'O', 'O', 'O', 'O', 'O', 'PAD', 'O']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
    "\n",
    "def encode_batch(batch):\n",
    "    # Tokenize list-of-words with alignment info\n",
    "    enc = tokenizer(\n",
    "        batch[\"tokens\"],\n",
    "        is_split_into_words=True,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        max_length=512,\n",
    "    )\n",
    "    all_labels = []\n",
    "    for i, tags in enumerate(batch[\"tags\"]):\n",
    "        word_ids = enc.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        prev_wid = None\n",
    "        for wid in word_ids:\n",
    "            if wid is None:\n",
    "                label_ids.append(-100)            # special tokens\n",
    "            elif wid != prev_wid:\n",
    "                label_ids.append(label2id[tags[wid]])  # first subword of this word\n",
    "            else:\n",
    "                label_ids.append(-100)            # subsequent subwords\n",
    "            prev_wid = wid\n",
    "        all_labels.append(label_ids)\n",
    "    enc[\"labels\"] = all_labels\n",
    "    return enc\n",
    "\n",
    "encoded = ds.map(\n",
    "    encode_batch,\n",
    "    batched=True,\n",
    "    remove_columns=[\"tokens\",\"tags\"],\n",
    "    desc=\"Tokenizing and aligning labels\",\n",
    ")\n",
    "\n",
    "print(encoded)\n",
    "row = encoded[\"train\"][0]\n",
    "print(\"Keys in encoded batch:\", row.keys())\n",
    "print(\"len(input_ids) vs len(labels):\", len(row[\"input_ids\"]), len(row[\"labels\"]))\n",
    "# How many labels are active (i.e., not -100) in this example?\n",
    "active = sum(1 for x in row[\"labels\"] if x != -100)\n",
    "print(\"Active labels in example row:\", active)\n",
    "print(\"First 20 labels (ids):\", row[\"labels\"][:20])\n",
    "print(\"First 20 label names:\", [id2label[i] if i!=-100 else \"PAD\" for i in row[\"labels\"][:20]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffab78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertForTokenClassification loaded with 13 labels.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoConfig\n",
    "\n",
    "MODEL_NAME = \"distilbert-base-cased\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(BIO_LABELS),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(MODEL_NAME, config=config)\n",
    "print(type(model).__name__, \"loaded with\", config.num_labels, \"labels.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fe93a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Winston\\AppData\\Local\\Temp\\ipykernel_42720\\893606672.py:52: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForTokenClassification, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    \"\"\"\n",
    "    p.predictions: (batch, seq_len, num_labels)\n",
    "    p.label_ids:   (batch, seq_len) with -100 for ignored positions\n",
    "    \"\"\"\n",
    "    preds = np.argmax(p.predictions, axis=2)\n",
    "    true_preds, true_labels = [], []\n",
    "    for pred, lab in zip(preds, p.label_ids):\n",
    "        # keep only positions where label != -100\n",
    "        keep = lab != -100\n",
    "        pred_ids = pred[keep]\n",
    "        lab_ids  = lab[keep]\n",
    "        true_preds.append([id2label[int(i)] for i in pred_ids])\n",
    "        true_labels.append([id2label[int(i)] for i in lab_ids])\n",
    "\n",
    "    results = seqeval.compute(predictions=true_preds, references=true_labels, zero_division=0)\n",
    "    # Flatten to a friendly dict: overall + per-entity F1\n",
    "    out = {\n",
    "        \"overall_precision\": results[\"overall_precision\"],\n",
    "        \"overall_recall\": results[\"overall_recall\"],\n",
    "        \"overall_f1\": results[\"overall_f1\"],\n",
    "    }\n",
    "    for ent, stats in results.items():\n",
    "        if isinstance(stats, dict) and \"f1\" in stats:\n",
    "            out[f\"f1_{ent}\"] = stats[\"f1\"]\n",
    "    return out\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"pii-ner-distilbert\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"overall_f1\",\n",
    "    report_to=\"none\",\n",
    "    logging_steps=50,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=encoded[\"train\"],\n",
    "    eval_dataset=encoded[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "print(\"Trainer ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2a2241",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Winston\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 21/750 11:03 < 7:04:21, 0.03 it/s, Epoch 0.08/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_out \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Training summary ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_out)\n",
      "File \u001b[1;32mc:\\Users\\Winston\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:2238\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2236\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2237\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2238\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   2239\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   2240\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[0;32m   2241\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[0;32m   2242\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[0;32m   2243\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Winston\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:2582\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2575\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2576\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2577\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2578\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2579\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2580\u001b[0m )\n\u001b[0;32m   2581\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2582\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[0;32m   2584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2585\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2586\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2587\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2588\u001b[0m ):\n\u001b[0;32m   2589\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2590\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\Winston\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:3796\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3793\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   3795\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 3796\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(model, inputs, num_items_in_batch\u001b[38;5;241m=\u001b[39mnum_items_in_batch)\n\u001b[0;32m   3798\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[0;32m   3799\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   3800\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   3802\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\Winston\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:3884\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3882\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[0;32m   3883\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[1;32m-> 3884\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m   3885\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   3886\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   3887\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Winston\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Winston\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Winston\\anaconda3\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:1129\u001b[0m, in \u001b[0;36mDistilBertForTokenClassification.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1124\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m \u001b[38;5;124;03m    Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\u001b[39;00m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1129\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistilbert(\n\u001b[0;32m   1130\u001b[0m     input_ids,\n\u001b[0;32m   1131\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   1132\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m   1133\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m   1134\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1135\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1136\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1137\u001b[0m )\n\u001b[0;32m   1139\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1141\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(sequence_output)\n",
      "File \u001b[1;32mc:\\Users\\Winston\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Winston\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Winston\\anaconda3\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:736\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_sdpa \u001b[38;5;129;01mand\u001b[39;00m head_mask_is_none \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m output_attentions:\n\u001b[0;32m    732\u001b[0m         attention_mask \u001b[38;5;241m=\u001b[39m _prepare_4d_attention_mask_for_sdpa(\n\u001b[0;32m    733\u001b[0m             attention_mask, embeddings\u001b[38;5;241m.\u001b[39mdtype, tgt_len\u001b[38;5;241m=\u001b[39minput_shape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    734\u001b[0m         )\n\u001b[1;32m--> 736\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(\n\u001b[0;32m    737\u001b[0m     x\u001b[38;5;241m=\u001b[39membeddings,\n\u001b[0;32m    738\u001b[0m     attn_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    739\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m    740\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    741\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m    742\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m    743\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Winston\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Winston\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Winston\\anaconda3\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:541\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[0;32m    539\u001b[0m     all_hidden_states \u001b[38;5;241m=\u001b[39m all_hidden_states \u001b[38;5;241m+\u001b[39m (hidden_state,)\n\u001b[1;32m--> 541\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[0;32m    542\u001b[0m     hidden_state,\n\u001b[0;32m    543\u001b[0m     attn_mask,\n\u001b[0;32m    544\u001b[0m     head_mask[i],\n\u001b[0;32m    545\u001b[0m     output_attentions,\n\u001b[0;32m    546\u001b[0m )\n\u001b[0;32m    548\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Users\\Winston\\anaconda3\\Lib\\site-packages\\transformers\\modeling_layers.py:93\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     90\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m---> 93\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Winston\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Winston\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Winston\\anaconda3\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:476\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[1;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;124;03mParameters:\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;124;03m    x: torch.tensor(bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m    torch.tensor(bs, seq_length, dim) The output of the transformer block contextualization.\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;66;03m# Self-Attention\u001b[39;00m\n\u001b[1;32m--> 476\u001b[0m sa_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(\n\u001b[0;32m    477\u001b[0m     query\u001b[38;5;241m=\u001b[39mx,\n\u001b[0;32m    478\u001b[0m     key\u001b[38;5;241m=\u001b[39mx,\n\u001b[0;32m    479\u001b[0m     value\u001b[38;5;241m=\u001b[39mx,\n\u001b[0;32m    480\u001b[0m     mask\u001b[38;5;241m=\u001b[39mattn_mask,\n\u001b[0;32m    481\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m    482\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    483\u001b[0m )\n\u001b[0;32m    484\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[0;32m    485\u001b[0m     sa_output, sa_weights \u001b[38;5;241m=\u001b[39m sa_output  \u001b[38;5;66;03m# (bs, seq_length, dim), (bs, n_heads, seq_length, seq_length)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Winston\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Winston\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Winston\\anaconda3\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:402\u001b[0m, in \u001b[0;36mDistilBertSdpaAttention.forward\u001b[1;34m(self, query, key, value, mask, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    399\u001b[0m     k \u001b[38;5;241m=\u001b[39m k\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    400\u001b[0m     v \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m--> 402\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mscaled_dot_product_attention(\n\u001b[0;32m    403\u001b[0m     q,\n\u001b[0;32m    404\u001b[0m     k,\n\u001b[0;32m    405\u001b[0m     v,\n\u001b[0;32m    406\u001b[0m     attn_mask\u001b[38;5;241m=\u001b[39mmask,\n\u001b[0;32m    407\u001b[0m     dropout_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout_prob \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m,\n\u001b[0;32m    408\u001b[0m     is_causal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    409\u001b[0m )\n\u001b[0;32m    411\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m unshape(attn_output)\n\u001b[0;32m    412\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_lin(attn_output)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train_out = trainer.train()\n",
    "# print(\"\\n=== Training summary ===\")\n",
    "# print(train_out)\n",
    "\n",
    "# eval_out = trainer.evaluate()\n",
    "# print(\"\\n=== Eval metrics (validation) ===\")\n",
    "# for k,v in sorted(eval_out.items()):\n",
    "#     if k.startswith(\"eval_\"):\n",
    "#         print(f\"{k}: {v:.4f}\" if isinstance(v, (int,float)) else f\"{k}: {v}\")\n",
    "\n",
    "# # ---------- SAVE THE MODEL & TOKENIZER ----------\n",
    "# SAVE_DIR = \"pii-ner-distilbert\"   # reuse output_dir or choose a new folder\n",
    "\n",
    "# trainer.save_model(SAVE_DIR)      # writes pytorch_model.bin + config.json (with id2label/label2id)\n",
    "# tokenizer.save_pretrained(SAVE_DIR)\n",
    "\n",
    "# # quick confirmation\n",
    "# print(\"\\nSaved files in\", SAVE_DIR, \":\\n\", os.listdir(SAVE_DIR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506e6499",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96aecfbed36a48ada3f354886ef922d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenize (speed mode):   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5261098f92524a3184750ea94f4cc0ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenize (speed mode):   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Winston\\AppData\\Local\\Temp\\ipykernel_42720\\1479944800.py:100: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer ready (speed mode).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='47' max='47' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [47/47 05:42, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training summary (speed mode) ===\n",
      "TrainOutput(global_step=47, training_loss=0.23462857591344954, metrics={'train_runtime': 380.3809, 'train_samples_per_second': 3.943, 'train_steps_per_second': 0.124, 'total_flos': 49004657280000.0, 'train_loss': 0.23462857591344954, 'epoch': 1.0})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Winston\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Eval metrics (validation, speed mode) ===\n",
      "eval_f1_ADDRESS: 0.8056\n",
      "eval_f1_EMAIL: 0.0000\n",
      "eval_f1_NAME: 0.6796\n",
      "eval_f1_PHONE: 0.0000\n",
      "eval_f1_URL_PERSONAL: 0.0000\n",
      "eval_f1_USERNAME: 0.0000\n",
      "eval_loss: 0.0326\n",
      "eval_overall_f1: 0.6621\n",
      "eval_overall_precision: 0.6175\n",
      "eval_overall_recall: 0.7137\n",
      "eval_runtime: 50.4945\n",
      "eval_samples_per_second: 5.9410\n",
      "eval_steps_per_second: 0.1980\n",
      "\n",
      "Model saved to: pii-ner-fast\n"
     ]
    }
   ],
   "source": [
    "# --- SPEED MODE SWITCHES ---\n",
    "MAX_LEN = 128            # shorter sequences = much faster\n",
    "TRAIN_ROWS = 1500        # small train subset\n",
    "VAL_ROWS = 300           # small val subset\n",
    "EPOCHS = 1               # quick pass\n",
    "BATCH = 32               # try 24/32; lower if you hit RAM issues\n",
    "\n",
    "# 1) Small, fast backbone\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForTokenClassification\n",
    "MODEL_NAME = \"distilbert-base-cased\"  # tiny & quick\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "config = AutoConfig.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(BIO_LABELS),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "model = AutoModelForTokenClassification.from_pretrained(MODEL_NAME, config=config)\n",
    "\n",
    "# 2) Re-tokenize with shorter max_length and first-subword labeling\n",
    "def encode_batch(batch):\n",
    "    enc = tokenizer(\n",
    "        batch[\"tokens\"],\n",
    "        is_split_into_words=True,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        max_length=MAX_LEN,\n",
    "    )\n",
    "    all_labels = []\n",
    "    for i, tags in enumerate(batch[\"tags\"]):\n",
    "        word_ids = enc.word_ids(batch_index=i)\n",
    "        label_ids, prev_wid = [], None\n",
    "        for wid in word_ids:\n",
    "            if wid is None:\n",
    "                label_ids.append(-100)\n",
    "            elif wid != prev_wid:\n",
    "                label_ids.append(label2id[tags[wid]])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            prev_wid = wid\n",
    "        all_labels.append(label_ids)\n",
    "    enc[\"labels\"] = all_labels\n",
    "    return enc\n",
    "\n",
    "from datasets import DatasetDict\n",
    "encoded_fast = DatasetDict({\n",
    "    \"train\": ds[\"train\"].select(range(min(TRAIN_ROWS, ds[\"train\"].num_rows))),\n",
    "    \"validation\": ds[\"validation\"].select(range(min(VAL_ROWS, ds[\"validation\"].num_rows))),\n",
    "}).map(encode_batch, batched=True, remove_columns=[\"tokens\",\"tags\"], desc=\"Tokenize (speed mode)\")\n",
    "\n",
    "# 3) Compat-safe Trainer args (only pass what your version supports)\n",
    "from transformers import DataCollatorForTokenClassification, TrainingArguments, Trainer\n",
    "import inspect, numpy as np, evaluate\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=2)\n",
    "    true_preds, true_labels = [], []\n",
    "    for pred, lab in zip(preds, p.label_ids):\n",
    "        keep = lab != -100\n",
    "        true_preds.append([id2label[int(i)] for i in pred[keep]])\n",
    "        true_labels.append([id2label[int(i)] for i in lab[keep]])\n",
    "    res = seqeval.compute(predictions=true_preds, references=true_labels, zero_division=0)\n",
    "    out = {\n",
    "        \"overall_precision\": res.get(\"overall_precision\", 0.0),\n",
    "        \"overall_recall\": res.get(\"overall_recall\", 0.0),\n",
    "        \"overall_f1\": res.get(\"overall_f1\", 0.0),\n",
    "    }\n",
    "    for ent, st in res.items():\n",
    "        if isinstance(st, dict) and \"f1\" in st:\n",
    "            out[f\"f1_{ent}\"] = st[\"f1\"]\n",
    "    return out\n",
    "\n",
    "sig = inspect.signature(TrainingArguments.__init__)\n",
    "params = set(sig.parameters.keys())\n",
    "\n",
    "kwargs = dict(\n",
    "    output_dir=\"pii-ner-fast\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=BATCH,\n",
    "    per_device_eval_batch_size=BATCH,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=0.0,      # less overhead\n",
    "    logging_steps=100,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# add optional args only if your installed version supports them\n",
    "if \"dataloader_num_workers\" in params:\n",
    "    kwargs[\"dataloader_num_workers\"] = 2\n",
    "if \"report_to\" in params:\n",
    "    kwargs[\"report_to\"] = \"none\"\n",
    "# we skip eval/save strategies during training; we’ll eval once after\n",
    "\n",
    "args = TrainingArguments(**kwargs)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=encoded_fast[\"train\"],\n",
    "    eval_dataset=encoded_fast[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Trainer ready (speed mode).\")\n",
    "\n",
    "# 4) Train quick, then evaluate once\n",
    "train_out = trainer.train()\n",
    "print(\"\\n=== Training summary (speed mode) ===\")\n",
    "print(train_out)\n",
    "\n",
    "eval_out = trainer.evaluate()\n",
    "print(\"\\n=== Eval metrics (validation, speed mode) ===\")\n",
    "for k, v in sorted(eval_out.items()):\n",
    "    if k.startswith(\"eval_\"):\n",
    "        print(f\"{k}: {v:.4f}\" if isinstance(v, (int,float)) else f\"{k}: {v}\")\n",
    "\n",
    "# Save a checkpoint you can test with Step 8\n",
    "trainer.save_model(\"pii-ner-fast\")\n",
    "tokenizer.save_pretrained(\"pii-ner-fast\")\n",
    "print(\"\\nModel saved to: pii-ner-fast\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee54320a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Winston\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:61: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tags: ['O', 'O', 'O', 'B-NAME', 'B-NAME', 'B-NAME', 'B-NAME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "offs: [(0, 2), (3, 7), (8, 10), (11, 18), (19, 26), (27, 28), (28, 31), (31, 34), (34, 36), (36, 37), (38, 40), (41, 48), (49, 51), (52, 57), (57, 58), (58, 59), (59, 60), (60, 67), (67, 68), (68, 71), (71, 72), (72, 76), (76, 79), (79, 80), (81, 84), (85, 88), (89, 94), (95, 97), (98, 100)]\n",
      "\n",
      "IN : My name is Winston Leonard Prayonggo. My website is https://example.com/winston. You can reach me at\n",
      "OUT: My name is [NAME]. My website is [URL_PERSONAL] You can reach me at\n",
      "SPN: [{'start': 11, 'end': 36, 'label': 'NAME'}, {'start': 52, 'end': 80, 'label': 'URL_PERSONAL'}]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "\n",
    "MODEL_DIR = \"./pii-ner-fast\"  # <- path to your saved model folder\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "mdl = AutoModelForTokenClassification.from_pretrained(MODEL_DIR)\n",
    "mdl.eval()\n",
    "\n",
    "import re\n",
    "\n",
    "def extend_spans_to_word_end(spans, text, labels={\"NAME\",\"ADDRESS\"}):\n",
    "    \"\"\"\n",
    "    If a span ends in the middle of a word (common with subword splits),\n",
    "    extend it to the end of that word. E.g., '[NAME]onggo' -> '[NAME]'.\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for s in spans:\n",
    "        if s[\"label\"] in labels:\n",
    "            end = s[\"end\"]\n",
    "            L = len(text)\n",
    "            # extend while next char is a word char or common name/addr joiners\n",
    "            while end < L and re.match(r\"[A-Za-z0-9'’-]\", text[end]):\n",
    "                end += 1\n",
    "            s = {\"start\": s[\"start\"], \"end\": end, \"label\": s[\"label\"]}\n",
    "        out.append(s)\n",
    "    return out\n",
    "\n",
    "\n",
    "def coalesce_same_label_spans(spans, text, max_gap_chars=2):\n",
    "    \"\"\"\n",
    "    Merge consecutive spans with the same label if the gap between them is tiny\n",
    "    (e.g., subword tail like 'oh' or '#11'), so '[ADDRESS]oh [ADDRESS]113' -> '[ADDRESS]'.\n",
    "    \"\"\"\n",
    "    if not spans:\n",
    "        return spans\n",
    "    spans = sorted(spans, key=lambda s: (s[\"start\"], s[\"end\"]))\n",
    "    merged = [spans[0]]\n",
    "\n",
    "    for s in spans[1:]:\n",
    "        prev = merged[-1]\n",
    "        if s[\"label\"] == prev[\"label\"]:\n",
    "            gap = text[prev[\"end\"]:s[\"start\"]]\n",
    "            # allow tiny tails: up to N alnum or -/#, with optional surrounding whitespace\n",
    "            if re.fullmatch(rf\"\\s*[-/#A-Za-z0-9]{{0,{max_gap_chars}}}\\s*\", gap):\n",
    "                # extend previous span to include gap + current span\n",
    "                prev[\"end\"] = s[\"end\"]\n",
    "                continue\n",
    "        merged.append(s)\n",
    "    return merged\n",
    "\n",
    "def predict_tags(text: str):\n",
    "    enc = tok(text, return_offsets_mapping=True, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        out = mdl(input_ids=enc[\"input_ids\"], attention_mask=enc[\"attention_mask\"])\n",
    "    pred_ids = out.logits.argmax(-1)[0].tolist()\n",
    "    tags = [mdl.config.id2label[int(i)] for i in pred_ids]\n",
    "\n",
    "    # drop special tokens (offsets == (0,0))\n",
    "    offsets = enc[\"offset_mapping\"][0].tolist()\n",
    "    toks, clean_tags, clean_offs = [], [], []\n",
    "    for (s,e), tag in zip(offsets, tags):\n",
    "        if s==0 and e==0:  # skip special tokens\n",
    "            continue\n",
    "        toks.append(text[s:e]); clean_tags.append(tag); clean_offs.append((s,e))\n",
    "    return toks, clean_tags, clean_offs\n",
    "\n",
    "\n",
    "# Merge BIO → character spans\n",
    "def bio_to_char_spans(offsets, tags):\n",
    "    spans, cur = [], None\n",
    "    for (s,e), tag in zip(offsets, tags):\n",
    "        if tag.startswith(\"B-\"):\n",
    "            if cur: spans.append(cur)\n",
    "            cur = {\"start\": s, \"end\": e, \"label\": tag.split(\"-\",1)[1]}\n",
    "        elif tag.startswith(\"I-\"):\n",
    "            ent = tag.split(\"-\",1)[1]\n",
    "            if cur and cur[\"label\"] == ent and s <= cur[\"end\"] + 1:\n",
    "                cur[\"end\"] = e\n",
    "            else:\n",
    "                cur = {\"start\": s, \"end\": e, \"label\": ent}\n",
    "        else:\n",
    "            if cur: spans.append(cur); cur = None\n",
    "    if cur: spans.append(cur)\n",
    "    return spans\n",
    "\n",
    "# High-precision regex for structured PII\n",
    "EMAIL_RE = re.compile(r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b\")\n",
    "PHONE_RE = re.compile(r\"\\b(?:\\+?\\d{1,3}[-.\\s]?)?(?:\\(?\\d{1,4}\\)?[-.\\s]?)?\\d{3,4}[-.\\s]?\\d{3,4}\\b\")\n",
    "URL_RE   = re.compile(r\"\\bhttps?://[^\\s]+\", re.I)\n",
    "HANDLE_RE= re.compile(r\"@\\w{1,32}\")\n",
    "\n",
    "def regex_spans(text):\n",
    "    out = []\n",
    "    for m in EMAIL_RE.finditer(text):\n",
    "        out.append({\"start\": m.start(), \"end\": m.end(), \"label\": \"EMAIL\"})\n",
    "    for m in PHONE_RE.finditer(text):\n",
    "        out.append({\"start\": m.start(), \"end\": m.end(), \"label\": \"PHONE\"})\n",
    "    for m in URL_RE.finditer(text):\n",
    "        out.append({\"start\": m.start(), \"end\": m.end(), \"label\": \"URL_PERSONAL\"})\n",
    "    for m in HANDLE_RE.finditer(text):\n",
    "        out.append({\"start\": m.start(), \"end\": m.end(), \"label\": \"USERNAME\"})\n",
    "    return out\n",
    "\n",
    "def ner_spans(text):\n",
    "    toks, tags, offs = predict_tags(text)\n",
    "    print(\"tags:\", tags)\n",
    "    print(\"offs:\", offs)\n",
    "    return bio_to_char_spans(offs, tags)\n",
    "\n",
    "def merge_spans(spans):\n",
    "    spans = sorted(spans, key=lambda s: (s[\"start\"], -(s[\"end\"]-s[\"start\"])))\n",
    "    merged = []\n",
    "    for s in spans:\n",
    "        if not merged or s[\"start\"] >= merged[-1][\"end\"]:\n",
    "            merged.append(s)\n",
    "        else:\n",
    "            if (s[\"end\"]-s[\"start\"]) > (merged[-1][\"end\"]-merged[-1][\"start\"]):\n",
    "                merged[-1] = s\n",
    "    return merged\n",
    "\n",
    "def redact(text, style=\"tags\"):\n",
    "    # regex for EMAIL/PHONE/URL/USERNAME + NER for NAME/ADDRESS (+ any extras the model finds)\n",
    "    r = regex_spans(text)\n",
    "    n = ner_spans(text)\n",
    "\n",
    "    keep = r[:]  # always keep structured regex hits\n",
    "    covered = {(s[\"start\"], s[\"end\"]) for s in r}\n",
    "    for s in n:\n",
    "        # Always include model-detected NAME/ADDRESS; include others if regex didn’t already catch\n",
    "        if s[\"label\"] in {\"NAME\",\"ADDRESS\"} or (s[\"start\"], s[\"end\"]) not in covered:\n",
    "            keep.append(s)\n",
    "\n",
    "    spans = merge_spans(keep)\n",
    "    spans = coalesce_same_label_spans(spans, text, max_gap_chars=2)\n",
    "    spans = extend_spans_to_word_end(spans, text, labels={\"NAME\",\"ADDRESS\"})\n",
    "\n",
    "    out, last = [], 0\n",
    "    for s in spans:\n",
    "        out.append(text[last:s[\"start\"]])\n",
    "        token = f\"[{s['label']}]\" if style==\"tags\" else \"█\"*(s[\"end\"]-s[\"start\"])\n",
    "        out.append(token)\n",
    "        last = s[\"end\"]\n",
    "    out.append(text[last:])\n",
    "    return \"\".join(out), spans\n",
    "\n",
    "# Try a few examples:\n",
    "samples = [\n",
    "    \"My name is Winston Leonard Prayonggo. My website is https://example.com/winston. You can reach me at\"\n",
    "]\n",
    "for s in samples:\n",
    "    red, spans = redact(s, style=\"tags\")\n",
    "    print(\"\\nIN :\", s)\n",
    "    print(\"OUT:\", red)\n",
    "    print(\"SPN:\", spans)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
